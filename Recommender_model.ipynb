{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d7e680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af6842c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>76561198153524465</th>\n",
       "      <th>76561198041864324</th>\n",
       "      <th>76561198110981529</th>\n",
       "      <th>76561198078042697</th>\n",
       "      <th>76561198072956091</th>\n",
       "      <th>76561198044215451</th>\n",
       "      <th>76561198117910816</th>\n",
       "      <th>76561198067369254</th>\n",
       "      <th>76561198095881349</th>\n",
       "      <th>76561198160606109</th>\n",
       "      <th>...</th>\n",
       "      <th>76561198064051588</th>\n",
       "      <th>76561198065637682</th>\n",
       "      <th>76561198093942856</th>\n",
       "      <th>76561198084283944</th>\n",
       "      <th>76561198068872077</th>\n",
       "      <th>76561198090802478</th>\n",
       "      <th>76561198068470798</th>\n",
       "      <th>76561198098128800</th>\n",
       "      <th>76561198098303003</th>\n",
       "      <th>76561198060478569</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>NaN</td>\n",
       "      <td>71.0</td>\n",
       "      <td>271784.0</td>\n",
       "      <td>370587.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68001.0</td>\n",
       "      <td>28686.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15850.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11081.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11848.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172470</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>422.0</td>\n",
       "      <td>2519.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>52631.0</td>\n",
       "      <td>156473.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>218577.0</td>\n",
       "      <td>56384.0</td>\n",
       "      <td>17022.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2826.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10009.0</td>\n",
       "      <td>102617.0</td>\n",
       "      <td>16021.0</td>\n",
       "      <td>9316.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578080</th>\n",
       "      <td>7881.0</td>\n",
       "      <td>1317.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105578.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063730</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335330</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21090</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259970</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57900</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337950</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>986 rows × 8691 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         76561198153524465  76561198041864324  76561198110981529  \\\n",
       "570                    NaN               71.0           271784.0   \n",
       "1172470                NaN             1740.0                NaN   \n",
       "730                52631.0           156473.0                NaN   \n",
       "578080              7881.0             1317.0                NaN   \n",
       "1063730                NaN                NaN                NaN   \n",
       "...                    ...                ...                ...   \n",
       "335330                 NaN                NaN                NaN   \n",
       "21090                  NaN                NaN                NaN   \n",
       "1259970                NaN                NaN                NaN   \n",
       "57900                  NaN                NaN                NaN   \n",
       "337950                 NaN                NaN                NaN   \n",
       "\n",
       "         76561198078042697  76561198072956091  76561198044215451  \\\n",
       "570               370587.0               30.0              127.0   \n",
       "1172470                NaN              422.0             2519.0   \n",
       "730               218577.0            56384.0            17022.0   \n",
       "578080                 0.0                0.0                NaN   \n",
       "1063730                NaN                NaN                NaN   \n",
       "...                    ...                ...                ...   \n",
       "335330                 NaN                NaN                NaN   \n",
       "21090                  NaN                NaN                NaN   \n",
       "1259970                NaN                NaN                NaN   \n",
       "57900                  NaN                NaN                NaN   \n",
       "337950                 NaN                NaN                NaN   \n",
       "\n",
       "         76561198117910816  76561198067369254  76561198095881349  \\\n",
       "570                    NaN            68001.0            28686.0   \n",
       "1172470                NaN                NaN                NaN   \n",
       "730                    NaN             2826.0                NaN   \n",
       "578080                 NaN           105578.0                NaN   \n",
       "1063730                NaN                NaN                NaN   \n",
       "...                    ...                ...                ...   \n",
       "335330                 NaN                NaN                NaN   \n",
       "21090                  NaN                NaN                NaN   \n",
       "1259970                NaN                NaN                NaN   \n",
       "57900                  NaN                NaN                NaN   \n",
       "337950                 NaN                NaN                NaN   \n",
       "\n",
       "         76561198160606109  ...  76561198064051588  76561198065637682  \\\n",
       "570                    NaN  ...               62.0                NaN   \n",
       "1172470                NaN  ...                NaN                NaN   \n",
       "730                    0.0  ...            10009.0           102617.0   \n",
       "578080                 0.0  ...             8200.0                0.0   \n",
       "1063730                NaN  ...                NaN                NaN   \n",
       "...                    ...  ...                ...                ...   \n",
       "335330                 NaN  ...                NaN                NaN   \n",
       "21090                  NaN  ...                NaN                NaN   \n",
       "1259970                NaN  ...                NaN                NaN   \n",
       "57900                  NaN  ...                NaN                NaN   \n",
       "337950                 NaN  ...                NaN                NaN   \n",
       "\n",
       "         76561198093942856  76561198084283944  76561198068872077  \\\n",
       "570                15850.0                NaN            11081.0   \n",
       "1172470                NaN                0.0                NaN   \n",
       "730                16021.0             9316.0              134.0   \n",
       "578080               187.0              129.0                NaN   \n",
       "1063730                NaN                NaN                NaN   \n",
       "...                    ...                ...                ...   \n",
       "335330                 NaN                NaN                NaN   \n",
       "21090                  NaN                NaN                NaN   \n",
       "1259970                NaN                NaN                NaN   \n",
       "57900                  NaN                NaN                NaN   \n",
       "337950                 NaN                NaN                NaN   \n",
       "\n",
       "         76561198090802478  76561198068470798  76561198098128800  \\\n",
       "570                    NaN                NaN               17.0   \n",
       "1172470                NaN                NaN                NaN   \n",
       "730                    NaN                NaN                NaN   \n",
       "578080                 0.0                NaN                NaN   \n",
       "1063730                NaN                NaN                NaN   \n",
       "...                    ...                ...                ...   \n",
       "335330                 NaN                NaN                NaN   \n",
       "21090                  NaN                NaN                NaN   \n",
       "1259970                NaN                NaN                NaN   \n",
       "57900                  NaN                NaN                NaN   \n",
       "337950                 NaN                NaN                NaN   \n",
       "\n",
       "         76561198098303003  76561198060478569  \n",
       "570                11848.0                NaN  \n",
       "1172470                NaN                NaN  \n",
       "730                    NaN                NaN  \n",
       "578080                 NaN                NaN  \n",
       "1063730                NaN                NaN  \n",
       "...                    ...                ...  \n",
       "335330                 NaN                NaN  \n",
       "21090                  NaN                NaN  \n",
       "1259970                NaN                NaN  \n",
       "57900                  NaN                NaN  \n",
       "337950                 NaN                NaN  \n",
       "\n",
       "[986 rows x 8691 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('testingdf2.pkl')\n",
    "df = df.replace(pd.NA, np.nan)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f26bb237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>76561198153524465</th>\n",
       "      <th>76561198041864324</th>\n",
       "      <th>76561198110981529</th>\n",
       "      <th>76561198078042697</th>\n",
       "      <th>76561198072956091</th>\n",
       "      <th>76561198044215451</th>\n",
       "      <th>76561198117910816</th>\n",
       "      <th>76561198067369254</th>\n",
       "      <th>76561198095881349</th>\n",
       "      <th>76561198160606109</th>\n",
       "      <th>...</th>\n",
       "      <th>76561198064051588</th>\n",
       "      <th>76561198065637682</th>\n",
       "      <th>76561198093942856</th>\n",
       "      <th>76561198084283944</th>\n",
       "      <th>76561198068872077</th>\n",
       "      <th>76561198090802478</th>\n",
       "      <th>76561198068470798</th>\n",
       "      <th>76561198098128800</th>\n",
       "      <th>76561198098303003</th>\n",
       "      <th>76561198060478569</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>NaN</td>\n",
       "      <td>71.0</td>\n",
       "      <td>271784.0</td>\n",
       "      <td>370587.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68001.0</td>\n",
       "      <td>28686.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15850.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11081.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11848.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172470</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>422.0</td>\n",
       "      <td>2519.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>52631.0</td>\n",
       "      <td>156473.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>218577.0</td>\n",
       "      <td>56384.0</td>\n",
       "      <td>17022.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2826.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10009.0</td>\n",
       "      <td>102617.0</td>\n",
       "      <td>16021.0</td>\n",
       "      <td>9316.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578080</th>\n",
       "      <td>7881.0</td>\n",
       "      <td>1317.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>105578.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>187.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063730</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335330</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21090</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259970</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57900</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337950</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>986 rows × 8691 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         76561198153524465  76561198041864324  76561198110981529  \\\n",
       "570                    NaN               71.0           271784.0   \n",
       "1172470                NaN             1740.0                NaN   \n",
       "730                52631.0           156473.0                NaN   \n",
       "578080              7881.0             1317.0                NaN   \n",
       "1063730                NaN                NaN                NaN   \n",
       "...                    ...                ...                ...   \n",
       "335330                 NaN                NaN                NaN   \n",
       "21090                  NaN                NaN                NaN   \n",
       "1259970                NaN                NaN                NaN   \n",
       "57900                  NaN                NaN                NaN   \n",
       "337950                 NaN                NaN                NaN   \n",
       "\n",
       "         76561198078042697  76561198072956091  76561198044215451  \\\n",
       "570               370587.0               30.0              127.0   \n",
       "1172470                NaN              422.0             2519.0   \n",
       "730               218577.0            56384.0            17022.0   \n",
       "578080                 NaN                NaN                NaN   \n",
       "1063730                NaN                NaN                NaN   \n",
       "...                    ...                ...                ...   \n",
       "335330                 NaN                NaN                NaN   \n",
       "21090                  NaN                NaN                NaN   \n",
       "1259970                NaN                NaN                NaN   \n",
       "57900                  NaN                NaN                NaN   \n",
       "337950                 NaN                NaN                NaN   \n",
       "\n",
       "         76561198117910816  76561198067369254  76561198095881349  \\\n",
       "570                    NaN            68001.0            28686.0   \n",
       "1172470                NaN                NaN                NaN   \n",
       "730                    NaN             2826.0                NaN   \n",
       "578080                 NaN           105578.0                NaN   \n",
       "1063730                NaN                NaN                NaN   \n",
       "...                    ...                ...                ...   \n",
       "335330                 NaN                NaN                NaN   \n",
       "21090                  NaN                NaN                NaN   \n",
       "1259970                NaN                NaN                NaN   \n",
       "57900                  NaN                NaN                NaN   \n",
       "337950                 NaN                NaN                NaN   \n",
       "\n",
       "         76561198160606109  ...  76561198064051588  76561198065637682  \\\n",
       "570                    NaN  ...               62.0                NaN   \n",
       "1172470                NaN  ...                NaN                NaN   \n",
       "730                    NaN  ...            10009.0           102617.0   \n",
       "578080                 NaN  ...             8200.0                NaN   \n",
       "1063730                NaN  ...                NaN                NaN   \n",
       "...                    ...  ...                ...                ...   \n",
       "335330                 NaN  ...                NaN                NaN   \n",
       "21090                  NaN  ...                NaN                NaN   \n",
       "1259970                NaN  ...                NaN                NaN   \n",
       "57900                  NaN  ...                NaN                NaN   \n",
       "337950                 NaN  ...                NaN                NaN   \n",
       "\n",
       "         76561198093942856  76561198084283944  76561198068872077  \\\n",
       "570                15850.0                NaN            11081.0   \n",
       "1172470                NaN                NaN                NaN   \n",
       "730                16021.0             9316.0              134.0   \n",
       "578080               187.0              129.0                NaN   \n",
       "1063730                NaN                NaN                NaN   \n",
       "...                    ...                ...                ...   \n",
       "335330                 NaN                NaN                NaN   \n",
       "21090                  NaN                NaN                NaN   \n",
       "1259970                NaN                NaN                NaN   \n",
       "57900                  NaN                NaN                NaN   \n",
       "337950                 NaN                NaN                NaN   \n",
       "\n",
       "         76561198090802478  76561198068470798  76561198098128800  \\\n",
       "570                    NaN                NaN               17.0   \n",
       "1172470                NaN                NaN                NaN   \n",
       "730                    NaN                NaN                NaN   \n",
       "578080                 NaN                NaN                NaN   \n",
       "1063730                NaN                NaN                NaN   \n",
       "...                    ...                ...                ...   \n",
       "335330                 NaN                NaN                NaN   \n",
       "21090                  NaN                NaN                NaN   \n",
       "1259970                NaN                NaN                NaN   \n",
       "57900                  NaN                NaN                NaN   \n",
       "337950                 NaN                NaN                NaN   \n",
       "\n",
       "         76561198098303003  76561198060478569  \n",
       "570                11848.0                NaN  \n",
       "1172470                NaN                NaN  \n",
       "730                    NaN                NaN  \n",
       "578080                 NaN                NaN  \n",
       "1063730                NaN                NaN  \n",
       "...                    ...                ...  \n",
       "335330                 NaN                NaN  \n",
       "21090                  NaN                NaN  \n",
       "1259970                NaN                NaN  \n",
       "57900                  NaN                NaN  \n",
       "337950                 NaN                NaN  \n",
       "\n",
       "[986 rows x 8691 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell removes all zeroes from the dataframe, if uncommented.\n",
    "\n",
    "df = df.replace(np.nan, 0)\n",
    "df = df.replace(0, np.nan)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3d0bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFRecommender():\n",
    "    \"\"\"\n",
    "    Matrix factorization recommender model object.\n",
    "    \n",
    "    Attributes:\n",
    "        game_weights (np.array): matrix of weights corresponding to user \n",
    "            profile weights for each game.\n",
    "        user_weights (np.array): matrix of weights corresponding to how to \n",
    "            represent each user as a linear combination of user profiles.\n",
    "        filled_entries (List[tuple(int)]): list of index pairs (i,j) of the\n",
    "            non-null entries of df. \n",
    "        lr (float): learning rate\n",
    "        l2 (float): size of the l2 penalty when fitting and predicting.\n",
    "        df (pd.DataFrame): pandas dataframe representing the sparse matrix\n",
    "            of data\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, df, k, lr=0.0002, l2=0.00001, val_split=0.15):\n",
    "        \"\"\"\n",
    "        Initializes the model.\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): sparse dataframe of user playtime info.\n",
    "            k (int): Number of user profiles for the model.\n",
    "            lr (float): learning rate for SGD.\n",
    "            l2 (float): l2 penalty for training the weights.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.game_weights = np.random.rand(df.shape[0],k)\n",
    "        self.user_weights = np.random.rand(k, df.shape[1])\n",
    "        self.filled_entries = []\n",
    "        self.lr = lr\n",
    "        self.l2 = l2\n",
    "        for i in range(df.shape[0]):\n",
    "            for j in range(df.shape[1]):\n",
    "                if not pd.isna(df.iat[i,j]):\n",
    "                    self.filled_entries.append((i,j))\n",
    "        self.fe_train, self.fe_valid = train_test_split(self.filled_entries, \n",
    "                                                       test_size=val_split)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def fit(self, epochs):\n",
    "        \"\"\"\n",
    "        Fits the model a certain number of epochs based on the df provided.\n",
    "        Args:\n",
    "            epochs (int): number of epochs to train the model\n",
    "        \"\"\"\n",
    "        for k in range(epochs):\n",
    "            print('Fitting epoch {}...'.format(k+1))\n",
    "            pred = np.matmul(self.game_weights, self.user_weights)\n",
    "            t_game_weights = np.copy(self.game_weights)\n",
    "            t_user_weights = np.copy(self.user_weights)\n",
    "            \n",
    "            # apply l2 penalty\n",
    "            self.game_weights = t_game_weights*(1 - self.lr*self.l2)\n",
    "            self.user_weights = t_user_weights*(1 - self.lr*self.l2)\n",
    "            \n",
    "            # Update with SGD\n",
    "            for i, j in self.fe_train:\n",
    "                for l in range(self.user_weights.shape[0]):\n",
    "                    diff = (self.df.iat[i,j]-pred[i,j])\n",
    "                    self.game_weights[i,l] += self.lr*t_user_weights[l,j]*diff\n",
    "                    self.user_weights[l,j] += self.lr*t_game_weights[i,l]*diff\n",
    "            print('Train MSE = {:.5f}     '.format(self.train_MSE())\n",
    "                 + 'Validation MSE = {:.5f}'.format(self.valid_MSE()))\n",
    "    \n",
    "    \n",
    "    def train_loss(self):\n",
    "        loss = 0\n",
    "        pred = np.matmul(self.game_weights, self.user_weights)\n",
    "        for i, j in self.fe_train:\n",
    "            # print(self.df.iat[i,j], pred[i,j])\n",
    "            loss += (self.df.iat[i,j] - pred[i,j])**2\n",
    "        return loss\n",
    "    \n",
    "    def valid_loss(self):\n",
    "        loss = 0\n",
    "        pred2 = np.matmul(self.game_weights, self.user_weights)\n",
    "        for i, j in self.fe_valid:\n",
    "            # print(self.df.iat[i,j], pred[i,j])\n",
    "            loss += (self.df.iat[i,j] - pred2[i,j])**2\n",
    "        return loss\n",
    "    \n",
    "    def train_MSE(self):\n",
    "        loss = 0\n",
    "        pred2 = np.matmul(self.game_weights, self.user_weights)\n",
    "        for i, j in self.fe_train:\n",
    "            # print(self.df.iat[i,j], pred[i,j])\n",
    "            loss += (self.df.iat[i,j] - pred2[i,j])**2\n",
    "        return loss/len(self.fe_train)\n",
    "    \n",
    "    def valid_MSE(self):\n",
    "        loss = 0\n",
    "        pred2 = np.matmul(self.game_weights, self.user_weights)\n",
    "        for i, j in self.fe_valid:\n",
    "            # print(self.df.iat[i,j], pred[i,j])\n",
    "            loss += (self.df.iat[i,j] - pred2[i,j])**2\n",
    "        return loss/len(self.fe_valid)\n",
    "    \n",
    "    def predict(self, user_data, epochs=100):\n",
    "        \"\"\"\n",
    "        Given the data for a user, returns the predicted playtime series.\n",
    "        \n",
    "        Args:\n",
    "            user_data (pd.series): sparse pandas series of user playtime\n",
    "        \n",
    "        Returns (pd.series): filled pandas series of predicted playtime.\n",
    "        \"\"\"\n",
    "        user_profile = np.random.rand(self.user_weights.shape[0], 1)\n",
    "        \n",
    "        filled_indices = []\n",
    "        for j in range(user_data.shape[0]):\n",
    "            if not pd.isna(user_data.iat[j]):\n",
    "                filled_indices.append(j)\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            #print('Fitting epoch {}...'.format(k+1))\n",
    "            pred2 = np.matmul(self.game_weights, user_profile)\n",
    "            #print(pred)\n",
    "            t_user_weights = np.copy(user_profile)\n",
    "            \n",
    "            # apply l2 penalty\n",
    "            user_profile = t_user_weights*(1 - self.lr*self.l2)\n",
    "            \n",
    "            # Update with SGD\n",
    "            for i in filled_indices:\n",
    "                for l in range(self.user_weights.shape[0]):\n",
    "                    diff = (user_data.iat[i]-pred2[i])\n",
    "                    user_profile[l] += self.lr*self.game_weights[i,l]*diff\n",
    "        \n",
    "        # return final prediction, converted to pandas\n",
    "        to_return = user_data.copy()\n",
    "        pred2 = np.matmul(self.game_weights, user_profile)\n",
    "        for j in range(to_return.shape[0]):\n",
    "            to_return.iat[j] = pred2[j]\n",
    "            \n",
    "        loss = 0\n",
    "        for i in filled_indices:\n",
    "            # print(self.df.iat[i,j], pred[i,j])\n",
    "            loss += (user_data.iat[i] - pred2[i])**2\n",
    "        print('MSE: {}'.format(loss/len(filled_indices)))\n",
    "        return to_return\n",
    "                \n",
    "                \n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f8fd3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>76561198153524465</th>\n",
       "      <th>76561198041864324</th>\n",
       "      <th>76561198110981529</th>\n",
       "      <th>76561198078042697</th>\n",
       "      <th>76561198072956091</th>\n",
       "      <th>76561198044215451</th>\n",
       "      <th>76561198117910816</th>\n",
       "      <th>76561198067369254</th>\n",
       "      <th>76561198095881349</th>\n",
       "      <th>76561198160606109</th>\n",
       "      <th>...</th>\n",
       "      <th>76561198064051588</th>\n",
       "      <th>76561198065637682</th>\n",
       "      <th>76561198093942856</th>\n",
       "      <th>76561198084283944</th>\n",
       "      <th>76561198068872077</th>\n",
       "      <th>76561198090802478</th>\n",
       "      <th>76561198068470798</th>\n",
       "      <th>76561198098128800</th>\n",
       "      <th>76561198098303003</th>\n",
       "      <th>76561198060478569</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.857332</td>\n",
       "      <td>5.434225</td>\n",
       "      <td>5.568891</td>\n",
       "      <td>1.491362</td>\n",
       "      <td>2.107210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.832522</td>\n",
       "      <td>4.457685</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.799341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.200057</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.044618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.255273</td>\n",
       "      <td>4.073682</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172470</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.240799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.626340</td>\n",
       "      <td>3.401401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>4.721250</td>\n",
       "      <td>5.194442</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.339606</td>\n",
       "      <td>4.751164</td>\n",
       "      <td>4.231036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.451326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000434</td>\n",
       "      <td>5.011224</td>\n",
       "      <td>4.204717</td>\n",
       "      <td>3.969276</td>\n",
       "      <td>2.130334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578080</th>\n",
       "      <td>3.896636</td>\n",
       "      <td>3.119915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.023578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.913867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.274158</td>\n",
       "      <td>2.113943</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063730</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335330</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21090</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259970</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57900</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337950</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>986 rows × 8691 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         76561198153524465  76561198041864324  76561198110981529  \\\n",
       "570                    NaN           1.857332           5.434225   \n",
       "1172470                NaN           3.240799                NaN   \n",
       "730               4.721250           5.194442                NaN   \n",
       "578080            3.896636           3.119915                NaN   \n",
       "1063730                NaN                NaN                NaN   \n",
       "...                    ...                ...                ...   \n",
       "335330                 NaN                NaN                NaN   \n",
       "21090                  NaN                NaN                NaN   \n",
       "1259970                NaN                NaN                NaN   \n",
       "57900                  NaN                NaN                NaN   \n",
       "337950                 NaN                NaN                NaN   \n",
       "\n",
       "         76561198078042697  76561198072956091  76561198044215451  \\\n",
       "570               5.568891           1.491362           2.107210   \n",
       "1172470                NaN           2.626340           3.401401   \n",
       "730               5.339606           4.751164           4.231036   \n",
       "578080                 NaN                NaN                NaN   \n",
       "1063730                NaN                NaN                NaN   \n",
       "...                    ...                ...                ...   \n",
       "335330                 NaN                NaN                NaN   \n",
       "21090                  NaN                NaN                NaN   \n",
       "1259970                NaN                NaN                NaN   \n",
       "57900                  NaN                NaN                NaN   \n",
       "337950                 NaN                NaN                NaN   \n",
       "\n",
       "         76561198117910816  76561198067369254  76561198095881349  \\\n",
       "570                    NaN           4.832522           4.457685   \n",
       "1172470                NaN                NaN                NaN   \n",
       "730                    NaN           3.451326                NaN   \n",
       "578080                 NaN           5.023578                NaN   \n",
       "1063730                NaN                NaN                NaN   \n",
       "...                    ...                ...                ...   \n",
       "335330                 NaN                NaN                NaN   \n",
       "21090                  NaN                NaN                NaN   \n",
       "1259970                NaN                NaN                NaN   \n",
       "57900                  NaN                NaN                NaN   \n",
       "337950                 NaN                NaN                NaN   \n",
       "\n",
       "         76561198160606109  ...  76561198064051588  76561198065637682  \\\n",
       "570                    NaN  ...           1.799341                NaN   \n",
       "1172470                NaN  ...                NaN                NaN   \n",
       "730                    NaN  ...           4.000434           5.011224   \n",
       "578080                 NaN  ...           3.913867                NaN   \n",
       "1063730                NaN  ...                NaN                NaN   \n",
       "...                    ...  ...                ...                ...   \n",
       "335330                 NaN  ...                NaN                NaN   \n",
       "21090                  NaN  ...                NaN                NaN   \n",
       "1259970                NaN  ...                NaN                NaN   \n",
       "57900                  NaN  ...                NaN                NaN   \n",
       "337950                 NaN  ...                NaN                NaN   \n",
       "\n",
       "         76561198093942856  76561198084283944  76561198068872077  \\\n",
       "570               4.200057                NaN           4.044618   \n",
       "1172470                NaN                NaN                NaN   \n",
       "730               4.204717           3.969276           2.130334   \n",
       "578080            2.274158           2.113943                NaN   \n",
       "1063730                NaN                NaN                NaN   \n",
       "...                    ...                ...                ...   \n",
       "335330                 NaN                NaN                NaN   \n",
       "21090                  NaN                NaN                NaN   \n",
       "1259970                NaN                NaN                NaN   \n",
       "57900                  NaN                NaN                NaN   \n",
       "337950                 NaN                NaN                NaN   \n",
       "\n",
       "         76561198090802478  76561198068470798  76561198098128800  \\\n",
       "570                    NaN                NaN           1.255273   \n",
       "1172470                NaN                NaN                NaN   \n",
       "730                    NaN                NaN                NaN   \n",
       "578080                 NaN                NaN                NaN   \n",
       "1063730                NaN                NaN                NaN   \n",
       "...                    ...                ...                ...   \n",
       "335330                 NaN                NaN                NaN   \n",
       "21090                  NaN                NaN                NaN   \n",
       "1259970                NaN                NaN                NaN   \n",
       "57900                  NaN                NaN                NaN   \n",
       "337950                 NaN                NaN                NaN   \n",
       "\n",
       "         76561198098303003  76561198060478569  \n",
       "570               4.073682                NaN  \n",
       "1172470                NaN                NaN  \n",
       "730                    NaN                NaN  \n",
       "578080                 NaN                NaN  \n",
       "1063730                NaN                NaN  \n",
       "...                    ...                ...  \n",
       "335330                 NaN                NaN  \n",
       "21090                  NaN                NaN  \n",
       "1259970                NaN                NaN  \n",
       "57900                  NaN                NaN  \n",
       "337950                 NaN                NaN  \n",
       "\n",
       "[986 rows x 8691 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the outliers in the data will prevent convergence unless we log-normalize first.\n",
    "\n",
    "log_df = df.applymap(lambda x: np.log10(x+1) if not np.isnan(x) else x)\n",
    "log_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b1f4015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with k=7.\n",
      "Fitting epoch 1...\n",
      "Train MSE = 1.59081     Validation MSE = 1.58047\n",
      "Fitting epoch 2...\n",
      "Train MSE = 1.40550     Validation MSE = 1.39673\n",
      "Fitting epoch 3...\n",
      "Train MSE = 1.28883     Validation MSE = 1.28153\n",
      "Fitting epoch 4...\n",
      "Train MSE = 1.20603     Validation MSE = 1.20003\n",
      "Fitting epoch 5...\n",
      "Train MSE = 1.14310     Validation MSE = 1.13828\n",
      "Fitting epoch 6...\n",
      "Train MSE = 1.09307     Validation MSE = 1.08932\n",
      "Fitting epoch 7...\n",
      "Train MSE = 1.05200     Validation MSE = 1.04924\n",
      "Fitting epoch 8...\n",
      "Train MSE = 1.01747     Validation MSE = 1.01563\n",
      "Fitting epoch 9...\n",
      "Train MSE = 0.98789     Validation MSE = 0.98693\n",
      "Fitting epoch 10...\n",
      "Train MSE = 0.96220     Validation MSE = 0.96206\n",
      "Fitting epoch 11...\n",
      "Train MSE = 0.93961     Validation MSE = 0.94026\n",
      "Fitting epoch 12...\n",
      "Train MSE = 0.91955     Validation MSE = 0.92096\n",
      "Fitting epoch 13...\n",
      "Train MSE = 0.90161     Validation MSE = 0.90373\n",
      "Fitting epoch 14...\n",
      "Train MSE = 0.88544     Validation MSE = 0.88825\n",
      "Fitting epoch 15...\n",
      "Train MSE = 0.87079     Validation MSE = 0.87426\n",
      "Fitting epoch 16...\n",
      "Train MSE = 0.85744     Validation MSE = 0.86154\n",
      "Fitting epoch 17...\n",
      "Train MSE = 0.84522     Validation MSE = 0.84994\n",
      "Fitting epoch 18...\n",
      "Train MSE = 0.83400     Validation MSE = 0.83931\n",
      "Fitting epoch 19...\n",
      "Train MSE = 0.82365     Validation MSE = 0.82953\n",
      "Fitting epoch 20...\n",
      "Train MSE = 0.81408     Validation MSE = 0.82051\n",
      "Fitting epoch 21...\n",
      "Train MSE = 0.80521     Validation MSE = 0.81217\n",
      "Fitting epoch 22...\n",
      "Train MSE = 0.79695     Validation MSE = 0.80443\n",
      "Fitting epoch 23...\n",
      "Train MSE = 0.78925     Validation MSE = 0.79724\n",
      "Fitting epoch 24...\n",
      "Train MSE = 0.78205     Validation MSE = 0.79053\n",
      "Fitting epoch 25...\n",
      "Train MSE = 0.77532     Validation MSE = 0.78427\n",
      "Fitting epoch 26...\n",
      "Train MSE = 0.76900     Validation MSE = 0.77841\n",
      "Fitting epoch 27...\n",
      "Train MSE = 0.76306     Validation MSE = 0.77292\n",
      "Fitting epoch 28...\n",
      "Train MSE = 0.75747     Validation MSE = 0.76776\n",
      "Fitting epoch 29...\n",
      "Train MSE = 0.75220     Validation MSE = 0.76291\n",
      "Fitting epoch 30...\n",
      "Train MSE = 0.74722     Validation MSE = 0.75835\n",
      "Fitting epoch 31...\n",
      "Train MSE = 0.74251     Validation MSE = 0.75405\n",
      "Fitting epoch 32...\n",
      "Train MSE = 0.73806     Validation MSE = 0.74998\n",
      "Fitting epoch 33...\n",
      "Train MSE = 0.73383     Validation MSE = 0.74614\n",
      "Fitting epoch 34...\n",
      "Train MSE = 0.72983     Validation MSE = 0.74251\n",
      "Fitting epoch 35...\n",
      "Train MSE = 0.72602     Validation MSE = 0.73907\n",
      "Fitting epoch 36...\n",
      "Train MSE = 0.72239     Validation MSE = 0.73581\n",
      "Fitting epoch 37...\n",
      "Train MSE = 0.71895     Validation MSE = 0.73271\n",
      "Fitting epoch 38...\n",
      "Train MSE = 0.71566     Validation MSE = 0.72976\n",
      "Fitting epoch 39...\n",
      "Train MSE = 0.71252     Validation MSE = 0.72696\n",
      "Fitting epoch 40...\n",
      "Train MSE = 0.70953     Validation MSE = 0.72430\n",
      "Fitting epoch 41...\n",
      "Train MSE = 0.70667     Validation MSE = 0.72176\n",
      "Fitting epoch 42...\n",
      "Train MSE = 0.70394     Validation MSE = 0.71934\n",
      "Fitting epoch 43...\n",
      "Train MSE = 0.70132     Validation MSE = 0.71703\n",
      "Fitting epoch 44...\n",
      "Train MSE = 0.69881     Validation MSE = 0.71482\n",
      "Fitting epoch 45...\n",
      "Train MSE = 0.69641     Validation MSE = 0.71272\n",
      "Fitting epoch 46...\n",
      "Train MSE = 0.69410     Validation MSE = 0.71070\n",
      "Fitting epoch 47...\n",
      "Train MSE = 0.69189     Validation MSE = 0.70877\n",
      "Fitting epoch 48...\n",
      "Train MSE = 0.68976     Validation MSE = 0.70693\n",
      "Fitting epoch 49...\n",
      "Train MSE = 0.68772     Validation MSE = 0.70516\n",
      "Fitting epoch 50...\n",
      "Train MSE = 0.68575     Validation MSE = 0.70346\n",
      "Fitting epoch 51...\n",
      "Train MSE = 0.68386     Validation MSE = 0.70183\n",
      "Fitting epoch 52...\n",
      "Train MSE = 0.68203     Validation MSE = 0.70027\n",
      "Fitting epoch 53...\n",
      "Train MSE = 0.68028     Validation MSE = 0.69877\n",
      "Fitting epoch 54...\n",
      "Train MSE = 0.67858     Validation MSE = 0.69733\n",
      "Fitting epoch 55...\n",
      "Train MSE = 0.67695     Validation MSE = 0.69595\n",
      "Fitting epoch 56...\n",
      "Train MSE = 0.67537     Validation MSE = 0.69461\n",
      "Fitting epoch 57...\n",
      "Train MSE = 0.67385     Validation MSE = 0.69333\n",
      "Fitting epoch 58...\n",
      "Train MSE = 0.67238     Validation MSE = 0.69209\n",
      "Fitting epoch 59...\n",
      "Train MSE = 0.67096     Validation MSE = 0.69090\n",
      "Fitting epoch 60...\n",
      "Train MSE = 0.66958     Validation MSE = 0.68976\n",
      "Fitting epoch 61...\n",
      "Train MSE = 0.66825     Validation MSE = 0.68865\n",
      "Fitting epoch 62...\n",
      "Train MSE = 0.66696     Validation MSE = 0.68758\n",
      "Fitting epoch 63...\n",
      "Train MSE = 0.66572     Validation MSE = 0.68655\n",
      "Fitting epoch 64...\n",
      "Train MSE = 0.66451     Validation MSE = 0.68556\n",
      "Fitting epoch 65...\n",
      "Train MSE = 0.66333     Validation MSE = 0.68460\n",
      "Fitting epoch 66...\n",
      "Train MSE = 0.66220     Validation MSE = 0.68367\n",
      "Fitting epoch 67...\n",
      "Train MSE = 0.66109     Validation MSE = 0.68277\n",
      "Fitting epoch 68...\n",
      "Train MSE = 0.66002     Validation MSE = 0.68190\n",
      "Fitting epoch 69...\n",
      "Train MSE = 0.65899     Validation MSE = 0.68106\n",
      "Fitting epoch 70...\n",
      "Train MSE = 0.65798     Validation MSE = 0.68025\n",
      "Fitting epoch 71...\n",
      "Train MSE = 0.65699     Validation MSE = 0.67946\n",
      "Fitting epoch 72...\n",
      "Train MSE = 0.65604     Validation MSE = 0.67870\n",
      "Fitting epoch 73...\n",
      "Train MSE = 0.65511     Validation MSE = 0.67796\n",
      "Fitting epoch 74...\n",
      "Train MSE = 0.65421     Validation MSE = 0.67725\n",
      "Fitting epoch 75...\n",
      "Train MSE = 0.65333     Validation MSE = 0.67655\n",
      "Fitting epoch 76...\n",
      "Train MSE = 0.65248     Validation MSE = 0.67588\n",
      "Fitting epoch 77...\n",
      "Train MSE = 0.65165     Validation MSE = 0.67523\n",
      "Fitting epoch 78...\n",
      "Train MSE = 0.65084     Validation MSE = 0.67459\n",
      "Fitting epoch 79...\n",
      "Train MSE = 0.65005     Validation MSE = 0.67398\n",
      "Fitting epoch 80...\n",
      "Train MSE = 0.64928     Validation MSE = 0.67338\n",
      "Fitting epoch 81...\n",
      "Train MSE = 0.64853     Validation MSE = 0.67280\n",
      "Fitting epoch 82...\n",
      "Train MSE = 0.64780     Validation MSE = 0.67223\n",
      "Fitting epoch 83...\n",
      "Train MSE = 0.64708     Validation MSE = 0.67169\n",
      "Fitting epoch 84...\n",
      "Train MSE = 0.64639     Validation MSE = 0.67115\n",
      "Fitting epoch 85...\n",
      "Train MSE = 0.64571     Validation MSE = 0.67064\n",
      "Fitting epoch 86...\n",
      "Train MSE = 0.64504     Validation MSE = 0.67013\n",
      "Fitting epoch 87...\n",
      "Train MSE = 0.64440     Validation MSE = 0.66964\n",
      "Fitting epoch 88...\n",
      "Train MSE = 0.64376     Validation MSE = 0.66916\n",
      "Fitting epoch 89...\n",
      "Train MSE = 0.64314     Validation MSE = 0.66870\n",
      "Fitting epoch 90...\n",
      "Train MSE = 0.64254     Validation MSE = 0.66825\n",
      "Fitting epoch 91...\n",
      "Train MSE = 0.64195     Validation MSE = 0.66781\n",
      "Fitting epoch 92...\n",
      "Train MSE = 0.64137     Validation MSE = 0.66738\n",
      "Fitting epoch 93...\n",
      "Train MSE = 0.64080     Validation MSE = 0.66696\n",
      "Fitting epoch 94...\n",
      "Train MSE = 0.64025     Validation MSE = 0.66655\n",
      "Fitting epoch 95...\n",
      "Train MSE = 0.63971     Validation MSE = 0.66615\n",
      "Fitting epoch 96...\n",
      "Train MSE = 0.63918     Validation MSE = 0.66577\n",
      "Fitting epoch 97...\n",
      "Train MSE = 0.63866     Validation MSE = 0.66539\n",
      "Fitting epoch 98...\n",
      "Train MSE = 0.63815     Validation MSE = 0.66502\n",
      "Fitting epoch 99...\n",
      "Train MSE = 0.63765     Validation MSE = 0.66466\n",
      "Fitting epoch 100...\n",
      "Train MSE = 0.63716     Validation MSE = 0.66431\n",
      "Fitting epoch 101...\n",
      "Train MSE = 0.63669     Validation MSE = 0.66396\n",
      "Fitting epoch 102...\n",
      "Train MSE = 0.63622     Validation MSE = 0.66363\n",
      "Fitting epoch 103...\n",
      "Train MSE = 0.63576     Validation MSE = 0.66330\n",
      "Fitting epoch 104...\n",
      "Train MSE = 0.63531     Validation MSE = 0.66298\n",
      "Fitting epoch 105...\n",
      "Train MSE = 0.63487     Validation MSE = 0.66267\n",
      "Fitting epoch 106...\n",
      "Train MSE = 0.63443     Validation MSE = 0.66237\n",
      "Fitting epoch 107...\n",
      "Train MSE = 0.63401     Validation MSE = 0.66207\n",
      "Fitting epoch 108...\n",
      "Train MSE = 0.63359     Validation MSE = 0.66178\n",
      "Fitting epoch 109...\n",
      "Train MSE = 0.63318     Validation MSE = 0.66149\n",
      "Fitting epoch 110...\n",
      "Train MSE = 0.63278     Validation MSE = 0.66122\n",
      "Fitting epoch 111...\n",
      "Train MSE = 0.63239     Validation MSE = 0.66094\n",
      "Fitting epoch 112...\n",
      "Train MSE = 0.63200     Validation MSE = 0.66068\n",
      "Fitting epoch 113...\n",
      "Train MSE = 0.63162     Validation MSE = 0.66042\n",
      "Fitting epoch 114...\n",
      "Train MSE = 0.63125     Validation MSE = 0.66016\n",
      "Fitting epoch 115...\n",
      "Train MSE = 0.63088     Validation MSE = 0.65992\n",
      "Fitting epoch 116...\n",
      "Train MSE = 0.63052     Validation MSE = 0.65967\n",
      "Fitting epoch 117...\n",
      "Train MSE = 0.63016     Validation MSE = 0.65943\n",
      "Fitting epoch 118...\n",
      "Train MSE = 0.62981     Validation MSE = 0.65920\n",
      "Fitting epoch 119...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE = 0.62947     Validation MSE = 0.65897\n",
      "Fitting epoch 120...\n",
      "Train MSE = 0.62913     Validation MSE = 0.65875\n",
      "Fitting epoch 121...\n",
      "Train MSE = 0.62880     Validation MSE = 0.65853\n",
      "Fitting epoch 122...\n",
      "Train MSE = 0.62848     Validation MSE = 0.65831\n",
      "Fitting epoch 123...\n",
      "Train MSE = 0.62816     Validation MSE = 0.65810\n",
      "Fitting epoch 124...\n",
      "Train MSE = 0.62784     Validation MSE = 0.65790\n",
      "Fitting epoch 125...\n",
      "Train MSE = 0.62753     Validation MSE = 0.65769\n",
      "Fitting epoch 126...\n",
      "Train MSE = 0.62723     Validation MSE = 0.65750\n",
      "Fitting epoch 127...\n",
      "Train MSE = 0.62692     Validation MSE = 0.65730\n",
      "Fitting epoch 128...\n",
      "Train MSE = 0.62663     Validation MSE = 0.65711\n",
      "Fitting epoch 129...\n",
      "Train MSE = 0.62634     Validation MSE = 0.65692\n",
      "Fitting epoch 130...\n",
      "Train MSE = 0.62605     Validation MSE = 0.65674\n",
      "Fitting epoch 131...\n",
      "Train MSE = 0.62577     Validation MSE = 0.65656\n",
      "Fitting epoch 132...\n",
      "Train MSE = 0.62549     Validation MSE = 0.65639\n",
      "Fitting epoch 133...\n",
      "Train MSE = 0.62522     Validation MSE = 0.65621\n",
      "Fitting epoch 134...\n",
      "Train MSE = 0.62495     Validation MSE = 0.65604\n",
      "Fitting epoch 135...\n",
      "Train MSE = 0.62468     Validation MSE = 0.65588\n",
      "Fitting epoch 136...\n",
      "Train MSE = 0.62442     Validation MSE = 0.65571\n",
      "Fitting epoch 137...\n",
      "Train MSE = 0.62416     Validation MSE = 0.65555\n",
      "Fitting epoch 138...\n",
      "Train MSE = 0.62390     Validation MSE = 0.65540\n",
      "Fitting epoch 139...\n",
      "Train MSE = 0.62365     Validation MSE = 0.65524\n",
      "Fitting epoch 140...\n",
      "Train MSE = 0.62340     Validation MSE = 0.65509\n",
      "Fitting epoch 141...\n",
      "Train MSE = 0.62316     Validation MSE = 0.65494\n",
      "Fitting epoch 142...\n",
      "Train MSE = 0.62292     Validation MSE = 0.65480\n",
      "Fitting epoch 143...\n",
      "Train MSE = 0.62268     Validation MSE = 0.65465\n",
      "Fitting epoch 144...\n",
      "Train MSE = 0.62245     Validation MSE = 0.65451\n",
      "Fitting epoch 145...\n",
      "Train MSE = 0.62222     Validation MSE = 0.65437\n",
      "Fitting epoch 146...\n",
      "Train MSE = 0.62199     Validation MSE = 0.65424\n",
      "Fitting epoch 147...\n",
      "Train MSE = 0.62176     Validation MSE = 0.65411\n",
      "Fitting epoch 148...\n",
      "Train MSE = 0.62154     Validation MSE = 0.65397\n",
      "Fitting epoch 149...\n",
      "Train MSE = 0.62132     Validation MSE = 0.65385\n",
      "Fitting epoch 150...\n",
      "Train MSE = 0.62111     Validation MSE = 0.65372\n",
      "Fitting epoch 151...\n",
      "Train MSE = 0.62089     Validation MSE = 0.65360\n",
      "Fitting epoch 152...\n",
      "Train MSE = 0.62068     Validation MSE = 0.65347\n",
      "Fitting epoch 153...\n",
      "Train MSE = 0.62047     Validation MSE = 0.65335\n",
      "Fitting epoch 154...\n",
      "Train MSE = 0.62027     Validation MSE = 0.65324\n",
      "Fitting epoch 155...\n",
      "Train MSE = 0.62007     Validation MSE = 0.65312\n",
      "Fitting epoch 156...\n",
      "Train MSE = 0.61986     Validation MSE = 0.65301\n",
      "Fitting epoch 157...\n",
      "Train MSE = 0.61967     Validation MSE = 0.65289\n",
      "Fitting epoch 158...\n",
      "Train MSE = 0.61947     Validation MSE = 0.65278\n",
      "Fitting epoch 159...\n",
      "Train MSE = 0.61928     Validation MSE = 0.65267\n",
      "Fitting epoch 160...\n",
      "Train MSE = 0.61909     Validation MSE = 0.65257\n",
      "Fitting epoch 161...\n",
      "Train MSE = 0.61890     Validation MSE = 0.65246\n",
      "Fitting epoch 162...\n",
      "Train MSE = 0.61871     Validation MSE = 0.65236\n",
      "Fitting epoch 163...\n",
      "Train MSE = 0.61853     Validation MSE = 0.65226\n",
      "Fitting epoch 164...\n",
      "Train MSE = 0.61835     Validation MSE = 0.65216\n",
      "Fitting epoch 165...\n",
      "Train MSE = 0.61817     Validation MSE = 0.65206\n",
      "Fitting epoch 166...\n",
      "Train MSE = 0.61799     Validation MSE = 0.65196\n",
      "Fitting epoch 167...\n",
      "Train MSE = 0.61781     Validation MSE = 0.65187\n",
      "Fitting epoch 168...\n",
      "Train MSE = 0.61764     Validation MSE = 0.65178\n",
      "Fitting epoch 169...\n",
      "Train MSE = 0.61747     Validation MSE = 0.65168\n",
      "Fitting epoch 170...\n",
      "Train MSE = 0.61730     Validation MSE = 0.65159\n",
      "Fitting epoch 171...\n",
      "Train MSE = 0.61713     Validation MSE = 0.65150\n",
      "Fitting epoch 172...\n",
      "Train MSE = 0.61696     Validation MSE = 0.65142\n",
      "Fitting epoch 173...\n",
      "Train MSE = 0.61680     Validation MSE = 0.65133\n",
      "Fitting epoch 174...\n",
      "Train MSE = 0.61663     Validation MSE = 0.65125\n",
      "Fitting epoch 175...\n",
      "Train MSE = 0.61647     Validation MSE = 0.65116\n",
      "Fitting epoch 176...\n",
      "Train MSE = 0.61631     Validation MSE = 0.65108\n",
      "Fitting epoch 177...\n",
      "Train MSE = 0.61616     Validation MSE = 0.65100\n",
      "Fitting epoch 178...\n",
      "Train MSE = 0.61600     Validation MSE = 0.65092\n",
      "Fitting epoch 179...\n",
      "Train MSE = 0.61585     Validation MSE = 0.65084\n",
      "Fitting epoch 180...\n",
      "Train MSE = 0.61569     Validation MSE = 0.65076\n",
      "Fitting epoch 181...\n",
      "Train MSE = 0.61554     Validation MSE = 0.65069\n",
      "Fitting epoch 182...\n",
      "Train MSE = 0.61539     Validation MSE = 0.65061\n",
      "Fitting epoch 183...\n",
      "Train MSE = 0.61525     Validation MSE = 0.65054\n",
      "Fitting epoch 184...\n",
      "Train MSE = 0.61510     Validation MSE = 0.65047\n",
      "Fitting epoch 185...\n",
      "Train MSE = 0.61495     Validation MSE = 0.65039\n",
      "Fitting epoch 186...\n",
      "Train MSE = 0.61481     Validation MSE = 0.65032\n",
      "Fitting epoch 187...\n",
      "Train MSE = 0.61467     Validation MSE = 0.65025\n",
      "Fitting epoch 188...\n",
      "Train MSE = 0.61453     Validation MSE = 0.65019\n",
      "Fitting epoch 189...\n",
      "Train MSE = 0.61439     Validation MSE = 0.65012\n",
      "Fitting epoch 190...\n",
      "Train MSE = 0.61425     Validation MSE = 0.65005\n",
      "Fitting epoch 191...\n",
      "Train MSE = 0.61411     Validation MSE = 0.64999\n",
      "Fitting epoch 192...\n",
      "Train MSE = 0.61398     Validation MSE = 0.64992\n",
      "Fitting epoch 193...\n",
      "Train MSE = 0.61384     Validation MSE = 0.64986\n",
      "Fitting epoch 194...\n",
      "Train MSE = 0.61371     Validation MSE = 0.64980\n",
      "Fitting epoch 195...\n",
      "Train MSE = 0.61358     Validation MSE = 0.64974\n",
      "Fitting epoch 196...\n",
      "Train MSE = 0.61345     Validation MSE = 0.64968\n",
      "Fitting epoch 197...\n",
      "Train MSE = 0.61332     Validation MSE = 0.64962\n",
      "Fitting epoch 198...\n",
      "Train MSE = 0.61319     Validation MSE = 0.64956\n",
      "Fitting epoch 199...\n",
      "Train MSE = 0.61306     Validation MSE = 0.64950\n",
      "Fitting epoch 200...\n",
      "Train MSE = 0.61294     Validation MSE = 0.64944\n",
      "Training model with k=4.\n",
      "Fitting epoch 1...\n",
      "Train MSE = 3.02822     Validation MSE = 3.01130\n",
      "Fitting epoch 2...\n",
      "Train MSE = 2.62473     Validation MSE = 2.61069\n",
      "Fitting epoch 3...\n",
      "Train MSE = 2.33695     Validation MSE = 2.32656\n",
      "Fitting epoch 4...\n",
      "Train MSE = 2.11509     Validation MSE = 2.10823\n",
      "Fitting epoch 5...\n",
      "Train MSE = 1.93686     Validation MSE = 1.93323\n",
      "Fitting epoch 6...\n",
      "Train MSE = 1.79012     Validation MSE = 1.78943\n",
      "Fitting epoch 7...\n",
      "Train MSE = 1.66728     Validation MSE = 1.66927\n",
      "Fitting epoch 8...\n",
      "Train MSE = 1.56317     Validation MSE = 1.56761\n",
      "Fitting epoch 9...\n",
      "Train MSE = 1.47405     Validation MSE = 1.48073\n",
      "Fitting epoch 10...\n",
      "Train MSE = 1.39714     Validation MSE = 1.40588\n",
      "Fitting epoch 11...\n",
      "Train MSE = 1.33028     Validation MSE = 1.34090\n",
      "Fitting epoch 12...\n",
      "Train MSE = 1.27177     Validation MSE = 1.28413\n",
      "Fitting epoch 13...\n",
      "Train MSE = 1.22027     Validation MSE = 1.23423\n",
      "Fitting epoch 14...\n",
      "Train MSE = 1.17469     Validation MSE = 1.19013\n",
      "Fitting epoch 15...\n",
      "Train MSE = 1.13414     Validation MSE = 1.15095\n",
      "Fitting epoch 16...\n",
      "Train MSE = 1.09791     Validation MSE = 1.11598\n",
      "Fitting epoch 17...\n",
      "Train MSE = 1.06538     Validation MSE = 1.08463\n",
      "Fitting epoch 18...\n",
      "Train MSE = 1.03605     Validation MSE = 1.05640\n",
      "Fitting epoch 19...\n",
      "Train MSE = 1.00952     Validation MSE = 1.03088\n",
      "Fitting epoch 20...\n",
      "Train MSE = 0.98543     Validation MSE = 1.00774\n",
      "Fitting epoch 21...\n",
      "Train MSE = 0.96348     Validation MSE = 0.98668\n",
      "Fitting epoch 22...\n",
      "Train MSE = 0.94341     Validation MSE = 0.96744\n",
      "Fitting epoch 23...\n",
      "Train MSE = 0.92502     Validation MSE = 0.94983\n",
      "Fitting epoch 24...\n",
      "Train MSE = 0.90812     Validation MSE = 0.93366\n",
      "Fitting epoch 25...\n",
      "Train MSE = 0.89254     Validation MSE = 0.91877\n",
      "Fitting epoch 26...\n",
      "Train MSE = 0.87816     Validation MSE = 0.90503\n",
      "Fitting epoch 27...\n",
      "Train MSE = 0.86484     Validation MSE = 0.89232\n",
      "Fitting epoch 28...\n",
      "Train MSE = 0.85249     Validation MSE = 0.88054\n",
      "Fitting epoch 29...\n",
      "Train MSE = 0.84100     Validation MSE = 0.86959\n",
      "Fitting epoch 30...\n",
      "Train MSE = 0.83031     Validation MSE = 0.85941\n",
      "Fitting epoch 31...\n",
      "Train MSE = 0.82034     Validation MSE = 0.84993\n",
      "Fitting epoch 32...\n",
      "Train MSE = 0.81102     Validation MSE = 0.84107\n",
      "Fitting epoch 33...\n",
      "Train MSE = 0.80230     Validation MSE = 0.83278\n",
      "Fitting epoch 34...\n",
      "Train MSE = 0.79412     Validation MSE = 0.82502\n",
      "Fitting epoch 35...\n",
      "Train MSE = 0.78645     Validation MSE = 0.81774\n",
      "Fitting epoch 36...\n",
      "Train MSE = 0.77924     Validation MSE = 0.81091\n",
      "Fitting epoch 37...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE = 0.77245     Validation MSE = 0.80448\n",
      "Fitting epoch 38...\n",
      "Train MSE = 0.76606     Validation MSE = 0.79843\n",
      "Fitting epoch 39...\n",
      "Train MSE = 0.76003     Validation MSE = 0.79272\n",
      "Fitting epoch 40...\n",
      "Train MSE = 0.75433     Validation MSE = 0.78734\n",
      "Fitting epoch 41...\n",
      "Train MSE = 0.74894     Validation MSE = 0.78225\n",
      "Fitting epoch 42...\n",
      "Train MSE = 0.74385     Validation MSE = 0.77744\n",
      "Fitting epoch 43...\n",
      "Train MSE = 0.73902     Validation MSE = 0.77289\n",
      "Fitting epoch 44...\n",
      "Train MSE = 0.73443     Validation MSE = 0.76858\n",
      "Fitting epoch 45...\n",
      "Train MSE = 0.73009     Validation MSE = 0.76449\n",
      "Fitting epoch 46...\n",
      "Train MSE = 0.72595     Validation MSE = 0.76060\n",
      "Fitting epoch 47...\n",
      "Train MSE = 0.72203     Validation MSE = 0.75691\n",
      "Fitting epoch 48...\n",
      "Train MSE = 0.71829     Validation MSE = 0.75340\n",
      "Fitting epoch 49...\n",
      "Train MSE = 0.71473     Validation MSE = 0.75006\n",
      "Fitting epoch 50...\n",
      "Train MSE = 0.71133     Validation MSE = 0.74688\n",
      "Fitting epoch 51...\n",
      "Train MSE = 0.70809     Validation MSE = 0.74385\n",
      "Fitting epoch 52...\n",
      "Train MSE = 0.70500     Validation MSE = 0.74096\n",
      "Fitting epoch 53...\n",
      "Train MSE = 0.70204     Validation MSE = 0.73821\n",
      "Fitting epoch 54...\n",
      "Train MSE = 0.69922     Validation MSE = 0.73557\n",
      "Fitting epoch 55...\n",
      "Train MSE = 0.69651     Validation MSE = 0.73305\n",
      "Fitting epoch 56...\n",
      "Train MSE = 0.69393     Validation MSE = 0.73065\n",
      "Fitting epoch 57...\n",
      "Train MSE = 0.69145     Validation MSE = 0.72834\n",
      "Fitting epoch 58...\n",
      "Train MSE = 0.68907     Validation MSE = 0.72614\n",
      "Fitting epoch 59...\n",
      "Train MSE = 0.68679     Validation MSE = 0.72402\n",
      "Fitting epoch 60...\n",
      "Train MSE = 0.68460     Validation MSE = 0.72200\n",
      "Fitting epoch 61...\n",
      "Train MSE = 0.68250     Validation MSE = 0.72006\n",
      "Fitting epoch 62...\n",
      "Train MSE = 0.68048     Validation MSE = 0.71819\n",
      "Fitting epoch 63...\n",
      "Train MSE = 0.67854     Validation MSE = 0.71640\n",
      "Fitting epoch 64...\n",
      "Train MSE = 0.67667     Validation MSE = 0.71468\n",
      "Fitting epoch 65...\n",
      "Train MSE = 0.67487     Validation MSE = 0.71303\n",
      "Fitting epoch 66...\n",
      "Train MSE = 0.67314     Validation MSE = 0.71144\n",
      "Fitting epoch 67...\n",
      "Train MSE = 0.67148     Validation MSE = 0.70991\n",
      "Fitting epoch 68...\n",
      "Train MSE = 0.66987     Validation MSE = 0.70844\n",
      "Fitting epoch 69...\n",
      "Train MSE = 0.66832     Validation MSE = 0.70702\n",
      "Fitting epoch 70...\n",
      "Train MSE = 0.66682     Validation MSE = 0.70566\n",
      "Fitting epoch 71...\n",
      "Train MSE = 0.66538     Validation MSE = 0.70434\n",
      "Fitting epoch 72...\n",
      "Train MSE = 0.66398     Validation MSE = 0.70307\n",
      "Fitting epoch 73...\n",
      "Train MSE = 0.66264     Validation MSE = 0.70185\n",
      "Fitting epoch 74...\n",
      "Train MSE = 0.66133     Validation MSE = 0.70067\n",
      "Fitting epoch 75...\n",
      "Train MSE = 0.66007     Validation MSE = 0.69953\n",
      "Fitting epoch 76...\n",
      "Train MSE = 0.65886     Validation MSE = 0.69843\n",
      "Fitting epoch 77...\n",
      "Train MSE = 0.65768     Validation MSE = 0.69737\n",
      "Fitting epoch 78...\n",
      "Train MSE = 0.65654     Validation MSE = 0.69634\n",
      "Fitting epoch 79...\n",
      "Train MSE = 0.65543     Validation MSE = 0.69534\n",
      "Fitting epoch 80...\n",
      "Train MSE = 0.65436     Validation MSE = 0.69438\n",
      "Fitting epoch 81...\n",
      "Train MSE = 0.65332     Validation MSE = 0.69345\n",
      "Fitting epoch 82...\n",
      "Train MSE = 0.65232     Validation MSE = 0.69255\n",
      "Fitting epoch 83...\n",
      "Train MSE = 0.65134     Validation MSE = 0.69168\n",
      "Fitting epoch 84...\n",
      "Train MSE = 0.65039     Validation MSE = 0.69084\n",
      "Fitting epoch 85...\n",
      "Train MSE = 0.64947     Validation MSE = 0.69002\n",
      "Fitting epoch 86...\n",
      "Train MSE = 0.64858     Validation MSE = 0.68923\n",
      "Fitting epoch 87...\n",
      "Train MSE = 0.64771     Validation MSE = 0.68846\n",
      "Fitting epoch 88...\n",
      "Train MSE = 0.64687     Validation MSE = 0.68772\n",
      "Fitting epoch 89...\n",
      "Train MSE = 0.64605     Validation MSE = 0.68700\n",
      "Fitting epoch 90...\n",
      "Train MSE = 0.64526     Validation MSE = 0.68630\n",
      "Fitting epoch 91...\n",
      "Train MSE = 0.64448     Validation MSE = 0.68562\n",
      "Fitting epoch 92...\n",
      "Train MSE = 0.64373     Validation MSE = 0.68496\n",
      "Fitting epoch 93...\n",
      "Train MSE = 0.64300     Validation MSE = 0.68431\n",
      "Fitting epoch 94...\n",
      "Train MSE = 0.64228     Validation MSE = 0.68369\n",
      "Fitting epoch 95...\n",
      "Train MSE = 0.64159     Validation MSE = 0.68309\n",
      "Fitting epoch 96...\n",
      "Train MSE = 0.64091     Validation MSE = 0.68250\n",
      "Fitting epoch 97...\n",
      "Train MSE = 0.64026     Validation MSE = 0.68193\n",
      "Fitting epoch 98...\n",
      "Train MSE = 0.63961     Validation MSE = 0.68137\n",
      "Fitting epoch 99...\n",
      "Train MSE = 0.63899     Validation MSE = 0.68083\n",
      "Fitting epoch 100...\n",
      "Train MSE = 0.63838     Validation MSE = 0.68031\n",
      "Fitting epoch 101...\n",
      "Train MSE = 0.63778     Validation MSE = 0.67980\n",
      "Fitting epoch 102...\n",
      "Train MSE = 0.63720     Validation MSE = 0.67930\n",
      "Fitting epoch 103...\n",
      "Train MSE = 0.63664     Validation MSE = 0.67881\n",
      "Fitting epoch 104...\n",
      "Train MSE = 0.63609     Validation MSE = 0.67834\n",
      "Fitting epoch 105...\n",
      "Train MSE = 0.63555     Validation MSE = 0.67788\n",
      "Fitting epoch 106...\n",
      "Train MSE = 0.63502     Validation MSE = 0.67743\n",
      "Fitting epoch 107...\n",
      "Train MSE = 0.63451     Validation MSE = 0.67700\n",
      "Fitting epoch 108...\n",
      "Train MSE = 0.63400     Validation MSE = 0.67657\n",
      "Fitting epoch 109...\n",
      "Train MSE = 0.63351     Validation MSE = 0.67616\n",
      "Fitting epoch 110...\n",
      "Train MSE = 0.63303     Validation MSE = 0.67576\n",
      "Fitting epoch 111...\n",
      "Train MSE = 0.63257     Validation MSE = 0.67536\n",
      "Fitting epoch 112...\n",
      "Train MSE = 0.63211     Validation MSE = 0.67498\n",
      "Fitting epoch 113...\n",
      "Train MSE = 0.63166     Validation MSE = 0.67460\n",
      "Fitting epoch 114...\n",
      "Train MSE = 0.63122     Validation MSE = 0.67424\n",
      "Fitting epoch 115...\n",
      "Train MSE = 0.63079     Validation MSE = 0.67388\n",
      "Fitting epoch 116...\n",
      "Train MSE = 0.63037     Validation MSE = 0.67353\n",
      "Fitting epoch 117...\n",
      "Train MSE = 0.62996     Validation MSE = 0.67319\n",
      "Fitting epoch 118...\n",
      "Train MSE = 0.62956     Validation MSE = 0.67286\n",
      "Fitting epoch 119...\n",
      "Train MSE = 0.62917     Validation MSE = 0.67254\n",
      "Fitting epoch 120...\n",
      "Train MSE = 0.62878     Validation MSE = 0.67222\n",
      "Fitting epoch 121...\n",
      "Train MSE = 0.62840     Validation MSE = 0.67191\n",
      "Fitting epoch 122...\n",
      "Train MSE = 0.62803     Validation MSE = 0.67161\n",
      "Fitting epoch 123...\n",
      "Train MSE = 0.62767     Validation MSE = 0.67131\n",
      "Fitting epoch 124...\n",
      "Train MSE = 0.62732     Validation MSE = 0.67102\n",
      "Fitting epoch 125...\n",
      "Train MSE = 0.62697     Validation MSE = 0.67074\n",
      "Fitting epoch 126...\n",
      "Train MSE = 0.62663     Validation MSE = 0.67047\n",
      "Fitting epoch 127...\n",
      "Train MSE = 0.62629     Validation MSE = 0.67020\n",
      "Fitting epoch 128...\n",
      "Train MSE = 0.62597     Validation MSE = 0.66993\n",
      "Fitting epoch 129...\n",
      "Train MSE = 0.62564     Validation MSE = 0.66967\n",
      "Fitting epoch 130...\n",
      "Train MSE = 0.62533     Validation MSE = 0.66942\n",
      "Fitting epoch 131...\n",
      "Train MSE = 0.62502     Validation MSE = 0.66917\n",
      "Fitting epoch 132...\n",
      "Train MSE = 0.62472     Validation MSE = 0.66893\n",
      "Fitting epoch 133...\n",
      "Train MSE = 0.62442     Validation MSE = 0.66869\n",
      "Fitting epoch 134...\n",
      "Train MSE = 0.62413     Validation MSE = 0.66846\n",
      "Fitting epoch 135...\n",
      "Train MSE = 0.62384     Validation MSE = 0.66824\n",
      "Fitting epoch 136...\n",
      "Train MSE = 0.62356     Validation MSE = 0.66801\n",
      "Fitting epoch 137...\n",
      "Train MSE = 0.62328     Validation MSE = 0.66780\n",
      "Fitting epoch 138...\n",
      "Train MSE = 0.62301     Validation MSE = 0.66758\n",
      "Fitting epoch 139...\n",
      "Train MSE = 0.62274     Validation MSE = 0.66737\n",
      "Fitting epoch 140...\n",
      "Train MSE = 0.62248     Validation MSE = 0.66717\n",
      "Fitting epoch 141...\n",
      "Train MSE = 0.62222     Validation MSE = 0.66697\n",
      "Fitting epoch 142...\n",
      "Train MSE = 0.62197     Validation MSE = 0.66677\n",
      "Fitting epoch 143...\n",
      "Train MSE = 0.62172     Validation MSE = 0.66658\n",
      "Fitting epoch 144...\n",
      "Train MSE = 0.62147     Validation MSE = 0.66639\n",
      "Fitting epoch 145...\n",
      "Train MSE = 0.62123     Validation MSE = 0.66621\n",
      "Fitting epoch 146...\n",
      "Train MSE = 0.62099     Validation MSE = 0.66602\n",
      "Fitting epoch 147...\n",
      "Train MSE = 0.62076     Validation MSE = 0.66585\n",
      "Fitting epoch 148...\n",
      "Train MSE = 0.62053     Validation MSE = 0.66567\n",
      "Fitting epoch 149...\n",
      "Train MSE = 0.62031     Validation MSE = 0.66550\n",
      "Fitting epoch 150...\n",
      "Train MSE = 0.62009     Validation MSE = 0.66533\n",
      "Fitting epoch 151...\n",
      "Train MSE = 0.61987     Validation MSE = 0.66517\n",
      "Fitting epoch 152...\n",
      "Train MSE = 0.61965     Validation MSE = 0.66501\n",
      "Fitting epoch 153...\n",
      "Train MSE = 0.61944     Validation MSE = 0.66485\n",
      "Fitting epoch 154...\n",
      "Train MSE = 0.61923     Validation MSE = 0.66469\n",
      "Fitting epoch 155...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE = 0.61903     Validation MSE = 0.66454\n",
      "Fitting epoch 156...\n",
      "Train MSE = 0.61883     Validation MSE = 0.66439\n",
      "Fitting epoch 157...\n",
      "Train MSE = 0.61863     Validation MSE = 0.66424\n",
      "Fitting epoch 158...\n",
      "Train MSE = 0.61844     Validation MSE = 0.66410\n",
      "Fitting epoch 159...\n",
      "Train MSE = 0.61824     Validation MSE = 0.66396\n",
      "Fitting epoch 160...\n",
      "Train MSE = 0.61805     Validation MSE = 0.66382\n",
      "Fitting epoch 161...\n",
      "Train MSE = 0.61787     Validation MSE = 0.66368\n",
      "Fitting epoch 162...\n",
      "Train MSE = 0.61768     Validation MSE = 0.66354\n",
      "Fitting epoch 163...\n",
      "Train MSE = 0.61750     Validation MSE = 0.66341\n",
      "Fitting epoch 164...\n",
      "Train MSE = 0.61732     Validation MSE = 0.66328\n",
      "Fitting epoch 165...\n",
      "Train MSE = 0.61715     Validation MSE = 0.66316\n",
      "Fitting epoch 166...\n",
      "Train MSE = 0.61697     Validation MSE = 0.66303\n",
      "Fitting epoch 167...\n",
      "Train MSE = 0.61680     Validation MSE = 0.66291\n",
      "Fitting epoch 168...\n",
      "Train MSE = 0.61664     Validation MSE = 0.66279\n",
      "Fitting epoch 169...\n",
      "Train MSE = 0.61647     Validation MSE = 0.66267\n",
      "Fitting epoch 170...\n",
      "Train MSE = 0.61631     Validation MSE = 0.66255\n",
      "Fitting epoch 171...\n",
      "Train MSE = 0.61614     Validation MSE = 0.66244\n",
      "Fitting epoch 172...\n",
      "Train MSE = 0.61598     Validation MSE = 0.66232\n",
      "Fitting epoch 173...\n",
      "Train MSE = 0.61583     Validation MSE = 0.66221\n",
      "Fitting epoch 174...\n",
      "Train MSE = 0.61567     Validation MSE = 0.66210\n",
      "Fitting epoch 175...\n",
      "Train MSE = 0.61552     Validation MSE = 0.66200\n",
      "Fitting epoch 176...\n",
      "Train MSE = 0.61537     Validation MSE = 0.66189\n",
      "Fitting epoch 177...\n",
      "Train MSE = 0.61522     Validation MSE = 0.66179\n",
      "Fitting epoch 178...\n",
      "Train MSE = 0.61507     Validation MSE = 0.66168\n",
      "Fitting epoch 179...\n",
      "Train MSE = 0.61493     Validation MSE = 0.66158\n",
      "Fitting epoch 180...\n",
      "Train MSE = 0.61479     Validation MSE = 0.66148\n",
      "Fitting epoch 181...\n",
      "Train MSE = 0.61464     Validation MSE = 0.66139\n",
      "Fitting epoch 182...\n",
      "Train MSE = 0.61451     Validation MSE = 0.66129\n",
      "Fitting epoch 183...\n",
      "Train MSE = 0.61437     Validation MSE = 0.66120\n",
      "Fitting epoch 184...\n",
      "Train MSE = 0.61423     Validation MSE = 0.66111\n",
      "Fitting epoch 185...\n",
      "Train MSE = 0.61410     Validation MSE = 0.66101\n",
      "Fitting epoch 186...\n",
      "Train MSE = 0.61397     Validation MSE = 0.66092\n",
      "Fitting epoch 187...\n",
      "Train MSE = 0.61384     Validation MSE = 0.66084\n",
      "Fitting epoch 188...\n",
      "Train MSE = 0.61371     Validation MSE = 0.66075\n",
      "Fitting epoch 189...\n",
      "Train MSE = 0.61358     Validation MSE = 0.66066\n",
      "Fitting epoch 190...\n",
      "Train MSE = 0.61345     Validation MSE = 0.66058\n",
      "Fitting epoch 191...\n",
      "Train MSE = 0.61333     Validation MSE = 0.66050\n",
      "Fitting epoch 192...\n",
      "Train MSE = 0.61321     Validation MSE = 0.66042\n",
      "Fitting epoch 193...\n",
      "Train MSE = 0.61308     Validation MSE = 0.66033\n",
      "Fitting epoch 194...\n",
      "Train MSE = 0.61296     Validation MSE = 0.66026\n",
      "Fitting epoch 195...\n",
      "Train MSE = 0.61285     Validation MSE = 0.66018\n",
      "Fitting epoch 196...\n",
      "Train MSE = 0.61273     Validation MSE = 0.66010\n",
      "Fitting epoch 197...\n",
      "Train MSE = 0.61261     Validation MSE = 0.66003\n",
      "Fitting epoch 198...\n",
      "Train MSE = 0.61250     Validation MSE = 0.65995\n",
      "Fitting epoch 199...\n",
      "Train MSE = 0.61238     Validation MSE = 0.65988\n",
      "Fitting epoch 200...\n",
      "Train MSE = 0.61227     Validation MSE = 0.65981\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for k in [7, 4]:\n",
    "    print('Training model with k={}.'.format(k))\n",
    "    clf = MFRecommender(log_df, k, lr=0.0001, l2=0)\n",
    "    clf.fit(200)\n",
    "    models.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39108ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a98555f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with k=20.\n",
      "Fitting epoch 1...\n",
      "Train MSE = 5.28455     Validation MSE = 5.29958\n",
      "Fitting epoch 2...\n",
      "Train MSE = 4.16548     Validation MSE = 4.18814\n",
      "Fitting epoch 3...\n",
      "Train MSE = 3.45600     Validation MSE = 3.48198\n",
      "Fitting epoch 4...\n",
      "Train MSE = 2.96329     Validation MSE = 2.99100\n",
      "Fitting epoch 5...\n",
      "Train MSE = 2.60225     Validation MSE = 2.63094\n",
      "Fitting epoch 6...\n",
      "Train MSE = 2.32774     Validation MSE = 2.35703\n",
      "Fitting epoch 7...\n",
      "Train MSE = 2.11320     Validation MSE = 2.14285\n",
      "Fitting epoch 8...\n",
      "Train MSE = 1.94184     Validation MSE = 1.97168\n",
      "Fitting epoch 9...\n",
      "Train MSE = 1.80250     Validation MSE = 1.83243\n",
      "Fitting epoch 10...\n",
      "Train MSE = 1.68750     Validation MSE = 1.71744\n",
      "Fitting epoch 11...\n",
      "Train MSE = 1.59136     Validation MSE = 1.62125\n",
      "Fitting epoch 12...\n",
      "Train MSE = 1.51008     Validation MSE = 1.53988\n",
      "Fitting epoch 13...\n",
      "Train MSE = 1.44068     Validation MSE = 1.47036\n",
      "Fitting epoch 14...\n",
      "Train MSE = 1.38090     Validation MSE = 1.41045\n",
      "Fitting epoch 15...\n",
      "Train MSE = 1.32899     Validation MSE = 1.35840\n",
      "Fitting epoch 16...\n",
      "Train MSE = 1.28360     Validation MSE = 1.31285\n",
      "Fitting epoch 17...\n",
      "Train MSE = 1.24364     Validation MSE = 1.27274\n",
      "Fitting epoch 18...\n",
      "Train MSE = 1.20825     Validation MSE = 1.23720\n",
      "Fitting epoch 19...\n",
      "Train MSE = 1.17674     Validation MSE = 1.20553\n",
      "Fitting epoch 20...\n",
      "Train MSE = 1.14853     Validation MSE = 1.17717\n",
      "Fitting epoch 21...\n",
      "Train MSE = 1.12315     Validation MSE = 1.15166\n",
      "Fitting epoch 22...\n",
      "Train MSE = 1.10023     Validation MSE = 1.12860\n",
      "Fitting epoch 23...\n",
      "Train MSE = 1.07943     Validation MSE = 1.10767\n",
      "Fitting epoch 24...\n",
      "Train MSE = 1.06049     Validation MSE = 1.08861\n",
      "Fitting epoch 25...\n",
      "Train MSE = 1.04317     Validation MSE = 1.07119\n",
      "Fitting epoch 26...\n",
      "Train MSE = 1.02729     Validation MSE = 1.05520\n",
      "Fitting epoch 27...\n",
      "Train MSE = 1.01267     Validation MSE = 1.04049\n",
      "Fitting epoch 28...\n",
      "Train MSE = 0.99918     Validation MSE = 1.02692\n",
      "Fitting epoch 29...\n",
      "Train MSE = 0.98668     Validation MSE = 1.01435\n",
      "Fitting epoch 30...\n",
      "Train MSE = 0.97508     Validation MSE = 1.00268\n",
      "Fitting epoch 31...\n",
      "Train MSE = 0.96428     Validation MSE = 0.99183\n",
      "Fitting epoch 32...\n",
      "Train MSE = 0.95421     Validation MSE = 0.98171\n",
      "Fitting epoch 33...\n",
      "Train MSE = 0.94478     Validation MSE = 0.97224\n",
      "Fitting epoch 34...\n",
      "Train MSE = 0.93595     Validation MSE = 0.96337\n",
      "Fitting epoch 35...\n",
      "Train MSE = 0.92764     Validation MSE = 0.95505\n",
      "Fitting epoch 36...\n",
      "Train MSE = 0.91983     Validation MSE = 0.94721\n",
      "Fitting epoch 37...\n",
      "Train MSE = 0.91245     Validation MSE = 0.93983\n",
      "Fitting epoch 38...\n",
      "Train MSE = 0.90549     Validation MSE = 0.93286\n",
      "Fitting epoch 39...\n",
      "Train MSE = 0.89889     Validation MSE = 0.92626\n",
      "Fitting epoch 40...\n",
      "Train MSE = 0.89263     Validation MSE = 0.92002\n",
      "Fitting epoch 41...\n",
      "Train MSE = 0.88669     Validation MSE = 0.91409\n",
      "Fitting epoch 42...\n",
      "Train MSE = 0.88103     Validation MSE = 0.90845\n",
      "Fitting epoch 43...\n",
      "Train MSE = 0.87564     Validation MSE = 0.90309\n",
      "Fitting epoch 44...\n",
      "Train MSE = 0.87050     Validation MSE = 0.89798\n",
      "Fitting epoch 45...\n",
      "Train MSE = 0.86558     Validation MSE = 0.89310\n",
      "Fitting epoch 46...\n",
      "Train MSE = 0.86088     Validation MSE = 0.88844\n",
      "Fitting epoch 47...\n",
      "Train MSE = 0.85637     Validation MSE = 0.88398\n",
      "Fitting epoch 48...\n",
      "Train MSE = 0.85205     Validation MSE = 0.87971\n",
      "Fitting epoch 49...\n",
      "Train MSE = 0.84790     Validation MSE = 0.87561\n",
      "Fitting epoch 50...\n",
      "Train MSE = 0.84391     Validation MSE = 0.87168\n",
      "Fitting epoch 51...\n",
      "Train MSE = 0.84007     Validation MSE = 0.86790\n",
      "Fitting epoch 52...\n",
      "Train MSE = 0.83637     Validation MSE = 0.86427\n",
      "Fitting epoch 53...\n",
      "Train MSE = 0.83281     Validation MSE = 0.86077\n",
      "Fitting epoch 54...\n",
      "Train MSE = 0.82936     Validation MSE = 0.85740\n",
      "Fitting epoch 55...\n",
      "Train MSE = 0.82604     Validation MSE = 0.85415\n",
      "Fitting epoch 56...\n",
      "Train MSE = 0.82282     Validation MSE = 0.85101\n",
      "Fitting epoch 57...\n",
      "Train MSE = 0.81971     Validation MSE = 0.84798\n",
      "Fitting epoch 58...\n",
      "Train MSE = 0.81670     Validation MSE = 0.84505\n",
      "Fitting epoch 59...\n",
      "Train MSE = 0.81377     Validation MSE = 0.84221\n",
      "Fitting epoch 60...\n",
      "Train MSE = 0.81094     Validation MSE = 0.83947\n",
      "Fitting epoch 61...\n",
      "Train MSE = 0.80819     Validation MSE = 0.83680\n",
      "Fitting epoch 62...\n",
      "Train MSE = 0.80552     Validation MSE = 0.83422\n",
      "Fitting epoch 63...\n",
      "Train MSE = 0.80292     Validation MSE = 0.83172\n",
      "Fitting epoch 64...\n",
      "Train MSE = 0.80040     Validation MSE = 0.82929\n",
      "Fitting epoch 65...\n",
      "Train MSE = 0.79794     Validation MSE = 0.82693\n",
      "Fitting epoch 66...\n",
      "Train MSE = 0.79555     Validation MSE = 0.82464\n",
      "Fitting epoch 67...\n",
      "Train MSE = 0.79322     Validation MSE = 0.82241\n",
      "Fitting epoch 68...\n",
      "Train MSE = 0.79095     Validation MSE = 0.82024\n",
      "Fitting epoch 69...\n",
      "Train MSE = 0.78874     Validation MSE = 0.81813\n",
      "Fitting epoch 70...\n",
      "Train MSE = 0.78658     Validation MSE = 0.81607\n",
      "Fitting epoch 71...\n",
      "Train MSE = 0.78447     Validation MSE = 0.81406\n",
      "Fitting epoch 72...\n",
      "Train MSE = 0.78241     Validation MSE = 0.81211\n",
      "Fitting epoch 73...\n",
      "Train MSE = 0.78040     Validation MSE = 0.81021\n",
      "Fitting epoch 74...\n",
      "Train MSE = 0.77843     Validation MSE = 0.80835\n",
      "Fitting epoch 75...\n",
      "Train MSE = 0.77651     Validation MSE = 0.80653\n",
      "Fitting epoch 76...\n",
      "Train MSE = 0.77463     Validation MSE = 0.80476\n",
      "Fitting epoch 77...\n",
      "Train MSE = 0.77279     Validation MSE = 0.80303\n",
      "Fitting epoch 78...\n",
      "Train MSE = 0.77099     Validation MSE = 0.80134\n",
      "Fitting epoch 79...\n",
      "Train MSE = 0.76923     Validation MSE = 0.79969\n",
      "Fitting epoch 80...\n",
      "Train MSE = 0.76750     Validation MSE = 0.79807\n",
      "Fitting epoch 81...\n",
      "Train MSE = 0.76581     Validation MSE = 0.79649\n",
      "Fitting epoch 82...\n",
      "Train MSE = 0.76415     Validation MSE = 0.79494\n",
      "Fitting epoch 83...\n",
      "Train MSE = 0.76252     Validation MSE = 0.79343\n",
      "Fitting epoch 84...\n",
      "Train MSE = 0.76092     Validation MSE = 0.79194\n",
      "Fitting epoch 85...\n",
      "Train MSE = 0.75936     Validation MSE = 0.79049\n",
      "Fitting epoch 86...\n",
      "Train MSE = 0.75782     Validation MSE = 0.78907\n",
      "Fitting epoch 87...\n",
      "Train MSE = 0.75631     Validation MSE = 0.78767\n",
      "Fitting epoch 88...\n",
      "Train MSE = 0.75483     Validation MSE = 0.78631\n",
      "Fitting epoch 89...\n",
      "Train MSE = 0.75338     Validation MSE = 0.78497\n",
      "Fitting epoch 90...\n",
      "Train MSE = 0.75195     Validation MSE = 0.78365\n",
      "Fitting epoch 91...\n",
      "Train MSE = 0.75054     Validation MSE = 0.78236\n",
      "Fitting epoch 92...\n",
      "Train MSE = 0.74916     Validation MSE = 0.78110\n",
      "Fitting epoch 93...\n",
      "Train MSE = 0.74781     Validation MSE = 0.77986\n",
      "Fitting epoch 94...\n",
      "Train MSE = 0.74647     Validation MSE = 0.77864\n",
      "Fitting epoch 95...\n",
      "Train MSE = 0.74516     Validation MSE = 0.77744\n",
      "Fitting epoch 96...\n",
      "Train MSE = 0.74387     Validation MSE = 0.77627\n",
      "Fitting epoch 97...\n",
      "Train MSE = 0.74260     Validation MSE = 0.77511\n",
      "Fitting epoch 98...\n",
      "Train MSE = 0.74135     Validation MSE = 0.77398\n",
      "Fitting epoch 99...\n",
      "Train MSE = 0.74012     Validation MSE = 0.77286\n",
      "Fitting epoch 100...\n",
      "Train MSE = 0.73891     Validation MSE = 0.77177\n",
      "Fitting epoch 101...\n",
      "Train MSE = 0.73772     Validation MSE = 0.77069\n",
      "Fitting epoch 102...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining model with k=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k))\n\u001b[1;32m      2\u001b[0m clf \u001b[38;5;241m=\u001b[39m MFRecommender(log_df, k, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00005\u001b[39m, l2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(clf)\n",
      "Cell \u001b[0;32mIn[4], line 64\u001b[0m, in \u001b[0;36mMFRecommender.fit\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     63\u001b[0m         diff \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miat[i,j]\u001b[38;5;241m-\u001b[39mpred[i,j])\n\u001b[0;32m---> 64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_weights[i,l] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr\u001b[38;5;241m*\u001b[39mt_user_weights[l,j]\u001b[38;5;241m*\u001b[39mdiff\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_weights[l,j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr\u001b[38;5;241m*\u001b[39mt_game_weights[i,l]\u001b[38;5;241m*\u001b[39mdiff\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain MSE = \u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m     \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_MSE())\n\u001b[1;32m     67\u001b[0m      \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation MSE = \u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_MSE()))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Training model with k={}.'.format(k))\n",
    "clf = MFRecommender(log_df, k, lr=0.00005, l2=0)\n",
    "clf.fit(200)\n",
    "models.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4dabb5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MFRecommender at 0x7fc1a01a3c40>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MFRecommender(log_df, 5, l2=0)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9c81324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting epoch 1...\n",
      "Train MSE = 0.63391     Validation MSE = 0.67366\n",
      "Fitting epoch 2...\n",
      "Train MSE = 0.63297     Validation MSE = 0.67288\n",
      "Fitting epoch 3...\n",
      "Train MSE = 0.63207     Validation MSE = 0.67216\n",
      "Fitting epoch 4...\n",
      "Train MSE = 0.63120     Validation MSE = 0.67147\n",
      "Fitting epoch 5...\n",
      "Train MSE = 0.63037     Validation MSE = 0.67081\n",
      "Fitting epoch 6...\n",
      "Train MSE = 0.62958     Validation MSE = 0.67019\n",
      "Fitting epoch 7...\n",
      "Train MSE = 0.62881     Validation MSE = 0.66960\n",
      "Fitting epoch 8...\n",
      "Train MSE = 0.62808     Validation MSE = 0.66903\n",
      "Fitting epoch 9...\n",
      "Train MSE = 0.62737     Validation MSE = 0.66849\n",
      "Fitting epoch 10...\n",
      "Train MSE = 0.62669     Validation MSE = 0.66797\n",
      "Fitting epoch 11...\n",
      "Train MSE = 0.62604     Validation MSE = 0.66748\n",
      "Fitting epoch 12...\n",
      "Train MSE = 0.62540     Validation MSE = 0.66701\n",
      "Fitting epoch 13...\n",
      "Train MSE = 0.62479     Validation MSE = 0.66656\n",
      "Fitting epoch 14...\n",
      "Train MSE = 0.62421     Validation MSE = 0.66612\n",
      "Fitting epoch 15...\n",
      "Train MSE = 0.62364     Validation MSE = 0.66571\n",
      "Fitting epoch 16...\n",
      "Train MSE = 0.62309     Validation MSE = 0.66531\n",
      "Fitting epoch 17...\n",
      "Train MSE = 0.62256     Validation MSE = 0.66493\n",
      "Fitting epoch 18...\n",
      "Train MSE = 0.62204     Validation MSE = 0.66456\n",
      "Fitting epoch 19...\n",
      "Train MSE = 0.62154     Validation MSE = 0.66421\n",
      "Fitting epoch 20...\n",
      "Train MSE = 0.62106     Validation MSE = 0.66387\n",
      "Fitting epoch 21...\n",
      "Train MSE = 0.62059     Validation MSE = 0.66354\n",
      "Fitting epoch 22...\n",
      "Train MSE = 0.62014     Validation MSE = 0.66323\n",
      "Fitting epoch 23...\n",
      "Train MSE = 0.61970     Validation MSE = 0.66293\n",
      "Fitting epoch 24...\n",
      "Train MSE = 0.61927     Validation MSE = 0.66264\n",
      "Fitting epoch 25...\n",
      "Train MSE = 0.61886     Validation MSE = 0.66236\n",
      "Fitting epoch 26...\n",
      "Train MSE = 0.61846     Validation MSE = 0.66209\n",
      "Fitting epoch 27...\n",
      "Train MSE = 0.61807     Validation MSE = 0.66183\n",
      "Fitting epoch 28...\n",
      "Train MSE = 0.61769     Validation MSE = 0.66158\n",
      "Fitting epoch 29...\n",
      "Train MSE = 0.61731     Validation MSE = 0.66134\n",
      "Fitting epoch 30...\n",
      "Train MSE = 0.61695     Validation MSE = 0.66111\n",
      "Fitting epoch 31...\n",
      "Train MSE = 0.61660     Validation MSE = 0.66089\n",
      "Fitting epoch 32...\n",
      "Train MSE = 0.61626     Validation MSE = 0.66067\n",
      "Fitting epoch 33...\n",
      "Train MSE = 0.61593     Validation MSE = 0.66046\n",
      "Fitting epoch 34...\n",
      "Train MSE = 0.61561     Validation MSE = 0.66026\n",
      "Fitting epoch 35...\n",
      "Train MSE = 0.61529     Validation MSE = 0.66006\n",
      "Fitting epoch 36...\n",
      "Train MSE = 0.61498     Validation MSE = 0.65988\n",
      "Fitting epoch 37...\n",
      "Train MSE = 0.61468     Validation MSE = 0.65969\n",
      "Fitting epoch 38...\n",
      "Train MSE = 0.61439     Validation MSE = 0.65952\n",
      "Fitting epoch 39...\n",
      "Train MSE = 0.61410     Validation MSE = 0.65935\n",
      "Fitting epoch 40...\n",
      "Train MSE = 0.61382     Validation MSE = 0.65918\n",
      "Fitting epoch 41...\n",
      "Train MSE = 0.61354     Validation MSE = 0.65902\n",
      "Fitting epoch 42...\n",
      "Train MSE = 0.61328     Validation MSE = 0.65887\n",
      "Fitting epoch 43...\n",
      "Train MSE = 0.61302     Validation MSE = 0.65872\n",
      "Fitting epoch 44...\n",
      "Train MSE = 0.61276     Validation MSE = 0.65857\n",
      "Fitting epoch 45...\n",
      "Train MSE = 0.61251     Validation MSE = 0.65843\n",
      "Fitting epoch 46...\n",
      "Train MSE = 0.61226     Validation MSE = 0.65830\n",
      "Fitting epoch 47...\n",
      "Train MSE = 0.61203     Validation MSE = 0.65817\n",
      "Fitting epoch 48...\n",
      "Train MSE = 0.61179     Validation MSE = 0.65804\n",
      "Fitting epoch 49...\n",
      "Train MSE = 0.61156     Validation MSE = 0.65792\n",
      "Fitting epoch 50...\n",
      "Train MSE = 0.61134     Validation MSE = 0.65780\n",
      "Fitting epoch 51...\n",
      "Train MSE = 0.61112     Validation MSE = 0.65768\n",
      "Fitting epoch 52...\n",
      "Train MSE = 0.61090     Validation MSE = 0.65757\n",
      "Fitting epoch 53...\n",
      "Train MSE = 0.61069     Validation MSE = 0.65746\n",
      "Fitting epoch 54...\n",
      "Train MSE = 0.61048     Validation MSE = 0.65735\n",
      "Fitting epoch 55...\n",
      "Train MSE = 0.61028     Validation MSE = 0.65725\n",
      "Fitting epoch 56...\n",
      "Train MSE = 0.61008     Validation MSE = 0.65715\n",
      "Fitting epoch 57...\n",
      "Train MSE = 0.60988     Validation MSE = 0.65705\n",
      "Fitting epoch 58...\n",
      "Train MSE = 0.60969     Validation MSE = 0.65696\n",
      "Fitting epoch 59...\n",
      "Train MSE = 0.60950     Validation MSE = 0.65687\n",
      "Fitting epoch 60...\n",
      "Train MSE = 0.60931     Validation MSE = 0.65678\n",
      "Fitting epoch 61...\n",
      "Train MSE = 0.60913     Validation MSE = 0.65669\n",
      "Fitting epoch 62...\n",
      "Train MSE = 0.60895     Validation MSE = 0.65661\n",
      "Fitting epoch 63...\n",
      "Train MSE = 0.60877     Validation MSE = 0.65652\n",
      "Fitting epoch 64...\n",
      "Train MSE = 0.60860     Validation MSE = 0.65645\n",
      "Fitting epoch 65...\n",
      "Train MSE = 0.60843     Validation MSE = 0.65637\n",
      "Fitting epoch 66...\n",
      "Train MSE = 0.60826     Validation MSE = 0.65629\n",
      "Fitting epoch 67...\n",
      "Train MSE = 0.60810     Validation MSE = 0.65622\n",
      "Fitting epoch 68...\n",
      "Train MSE = 0.60794     Validation MSE = 0.65615\n",
      "Fitting epoch 69...\n",
      "Train MSE = 0.60778     Validation MSE = 0.65608\n",
      "Fitting epoch 70...\n",
      "Train MSE = 0.60762     Validation MSE = 0.65602\n",
      "Fitting epoch 71...\n",
      "Train MSE = 0.60747     Validation MSE = 0.65595\n",
      "Fitting epoch 72...\n",
      "Train MSE = 0.60731     Validation MSE = 0.65589\n",
      "Fitting epoch 73...\n",
      "Train MSE = 0.60716     Validation MSE = 0.65583\n",
      "Fitting epoch 74...\n",
      "Train MSE = 0.60702     Validation MSE = 0.65577\n",
      "Fitting epoch 75...\n",
      "Train MSE = 0.60687     Validation MSE = 0.65571\n",
      "Fitting epoch 76...\n",
      "Train MSE = 0.60673     Validation MSE = 0.65566\n",
      "Fitting epoch 77...\n",
      "Train MSE = 0.60659     Validation MSE = 0.65560\n",
      "Fitting epoch 78...\n",
      "Train MSE = 0.60645     Validation MSE = 0.65555\n",
      "Fitting epoch 79...\n",
      "Train MSE = 0.60631     Validation MSE = 0.65550\n",
      "Fitting epoch 80...\n",
      "Train MSE = 0.60618     Validation MSE = 0.65545\n",
      "Fitting epoch 81...\n",
      "Train MSE = 0.60604     Validation MSE = 0.65540\n",
      "Fitting epoch 82...\n",
      "Train MSE = 0.60591     Validation MSE = 0.65535\n",
      "Fitting epoch 83...\n",
      "Train MSE = 0.60578     Validation MSE = 0.65531\n",
      "Fitting epoch 84...\n",
      "Train MSE = 0.60566     Validation MSE = 0.65526\n",
      "Fitting epoch 85...\n",
      "Train MSE = 0.60553     Validation MSE = 0.65522\n",
      "Fitting epoch 86...\n",
      "Train MSE = 0.60541     Validation MSE = 0.65518\n",
      "Fitting epoch 87...\n",
      "Train MSE = 0.60528     Validation MSE = 0.65514\n",
      "Fitting epoch 88...\n",
      "Train MSE = 0.60516     Validation MSE = 0.65510\n",
      "Fitting epoch 89...\n",
      "Train MSE = 0.60504     Validation MSE = 0.65506\n",
      "Fitting epoch 90...\n",
      "Train MSE = 0.60492     Validation MSE = 0.65502\n",
      "Fitting epoch 91...\n",
      "Train MSE = 0.60481     Validation MSE = 0.65498\n",
      "Fitting epoch 92...\n",
      "Train MSE = 0.60469     Validation MSE = 0.65495\n",
      "Fitting epoch 93...\n",
      "Train MSE = 0.60458     Validation MSE = 0.65491\n",
      "Fitting epoch 94...\n",
      "Train MSE = 0.60447     Validation MSE = 0.65488\n",
      "Fitting epoch 95...\n",
      "Train MSE = 0.60436     Validation MSE = 0.65485\n",
      "Fitting epoch 96...\n",
      "Train MSE = 0.60425     Validation MSE = 0.65482\n",
      "Fitting epoch 97...\n",
      "Train MSE = 0.60414     Validation MSE = 0.65479\n",
      "Fitting epoch 98...\n",
      "Train MSE = 0.60403     Validation MSE = 0.65476\n",
      "Fitting epoch 99...\n",
      "Train MSE = 0.60393     Validation MSE = 0.65473\n",
      "Fitting epoch 100...\n",
      "Train MSE = 0.60382     Validation MSE = 0.65470\n",
      "Fitting epoch 101...\n",
      "Train MSE = 0.60372     Validation MSE = 0.65467\n",
      "Fitting epoch 102...\n",
      "Train MSE = 0.60361     Validation MSE = 0.65465\n",
      "Fitting epoch 103...\n",
      "Train MSE = 0.60351     Validation MSE = 0.65462\n",
      "Fitting epoch 104...\n",
      "Train MSE = 0.60341     Validation MSE = 0.65460\n",
      "Fitting epoch 105...\n",
      "Train MSE = 0.60331     Validation MSE = 0.65457\n",
      "Fitting epoch 106...\n",
      "Train MSE = 0.60322     Validation MSE = 0.65455\n",
      "Fitting epoch 107...\n",
      "Train MSE = 0.60312     Validation MSE = 0.65453\n",
      "Fitting epoch 108...\n",
      "Train MSE = 0.60302     Validation MSE = 0.65451\n",
      "Fitting epoch 109...\n",
      "Train MSE = 0.60293     Validation MSE = 0.65449\n",
      "Fitting epoch 110...\n",
      "Train MSE = 0.60283     Validation MSE = 0.65447\n",
      "Fitting epoch 111...\n",
      "Train MSE = 0.60274     Validation MSE = 0.65445\n",
      "Fitting epoch 112...\n",
      "Train MSE = 0.60265     Validation MSE = 0.65443\n",
      "Fitting epoch 113...\n",
      "Train MSE = 0.60256     Validation MSE = 0.65441\n",
      "Fitting epoch 114...\n",
      "Train MSE = 0.60246     Validation MSE = 0.65439\n",
      "Fitting epoch 115...\n",
      "Train MSE = 0.60237     Validation MSE = 0.65438\n",
      "Fitting epoch 116...\n",
      "Train MSE = 0.60229     Validation MSE = 0.65436\n",
      "Fitting epoch 117...\n",
      "Train MSE = 0.60220     Validation MSE = 0.65435\n",
      "Fitting epoch 118...\n",
      "Train MSE = 0.60211     Validation MSE = 0.65433\n",
      "Fitting epoch 119...\n",
      "Train MSE = 0.60202     Validation MSE = 0.65432\n",
      "Fitting epoch 120...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE = 0.60194     Validation MSE = 0.65430\n",
      "Fitting epoch 121...\n",
      "Train MSE = 0.60185     Validation MSE = 0.65429\n",
      "Fitting epoch 122...\n",
      "Train MSE = 0.60177     Validation MSE = 0.65428\n",
      "Fitting epoch 123...\n",
      "Train MSE = 0.60169     Validation MSE = 0.65426\n",
      "Fitting epoch 124...\n",
      "Train MSE = 0.60160     Validation MSE = 0.65425\n",
      "Fitting epoch 125...\n",
      "Train MSE = 0.60152     Validation MSE = 0.65424\n",
      "Fitting epoch 126...\n",
      "Train MSE = 0.60144     Validation MSE = 0.65423\n",
      "Fitting epoch 127...\n",
      "Train MSE = 0.60136     Validation MSE = 0.65422\n",
      "Fitting epoch 128...\n",
      "Train MSE = 0.60128     Validation MSE = 0.65421\n",
      "Fitting epoch 129...\n",
      "Train MSE = 0.60120     Validation MSE = 0.65420\n",
      "Fitting epoch 130...\n",
      "Train MSE = 0.60112     Validation MSE = 0.65419\n",
      "Fitting epoch 131...\n",
      "Train MSE = 0.60104     Validation MSE = 0.65418\n",
      "Fitting epoch 132...\n",
      "Train MSE = 0.60096     Validation MSE = 0.65418\n",
      "Fitting epoch 133...\n",
      "Train MSE = 0.60089     Validation MSE = 0.65417\n",
      "Fitting epoch 134...\n",
      "Train MSE = 0.60081     Validation MSE = 0.65416\n",
      "Fitting epoch 135...\n",
      "Train MSE = 0.60073     Validation MSE = 0.65415\n",
      "Fitting epoch 136...\n",
      "Train MSE = 0.60066     Validation MSE = 0.65415\n",
      "Fitting epoch 137...\n",
      "Train MSE = 0.60058     Validation MSE = 0.65414\n",
      "Fitting epoch 138...\n",
      "Train MSE = 0.60051     Validation MSE = 0.65414\n",
      "Fitting epoch 139...\n",
      "Train MSE = 0.60044     Validation MSE = 0.65413\n",
      "Fitting epoch 140...\n",
      "Train MSE = 0.60036     Validation MSE = 0.65413\n",
      "Fitting epoch 141...\n",
      "Train MSE = 0.60029     Validation MSE = 0.65412\n",
      "Fitting epoch 142...\n",
      "Train MSE = 0.60022     Validation MSE = 0.65412\n",
      "Fitting epoch 143...\n",
      "Train MSE = 0.60014     Validation MSE = 0.65412\n",
      "Fitting epoch 144...\n",
      "Train MSE = 0.60007     Validation MSE = 0.65411\n",
      "Fitting epoch 145...\n",
      "Train MSE = 0.60000     Validation MSE = 0.65411\n",
      "Fitting epoch 146...\n",
      "Train MSE = 0.59993     Validation MSE = 0.65411\n",
      "Fitting epoch 147...\n",
      "Train MSE = 0.59986     Validation MSE = 0.65411\n",
      "Fitting epoch 148...\n",
      "Train MSE = 0.59979     Validation MSE = 0.65410\n",
      "Fitting epoch 149...\n",
      "Train MSE = 0.59972     Validation MSE = 0.65410\n",
      "Fitting epoch 150...\n",
      "Train MSE = 0.59965     Validation MSE = 0.65410\n",
      "Fitting epoch 151...\n",
      "Train MSE = 0.59959     Validation MSE = 0.65410\n",
      "Fitting epoch 152...\n",
      "Train MSE = 0.59952     Validation MSE = 0.65410\n",
      "Fitting epoch 153...\n",
      "Train MSE = 0.59945     Validation MSE = 0.65410\n",
      "Fitting epoch 154...\n",
      "Train MSE = 0.59938     Validation MSE = 0.65410\n",
      "Fitting epoch 155...\n",
      "Train MSE = 0.59932     Validation MSE = 0.65410\n",
      "Fitting epoch 156...\n",
      "Train MSE = 0.59925     Validation MSE = 0.65410\n",
      "Fitting epoch 157...\n",
      "Train MSE = 0.59918     Validation MSE = 0.65410\n",
      "Fitting epoch 158...\n",
      "Train MSE = 0.59912     Validation MSE = 0.65410\n",
      "Fitting epoch 159...\n",
      "Train MSE = 0.59905     Validation MSE = 0.65411\n",
      "Fitting epoch 160...\n",
      "Train MSE = 0.59899     Validation MSE = 0.65411\n",
      "Fitting epoch 161...\n",
      "Train MSE = 0.59892     Validation MSE = 0.65411\n",
      "Fitting epoch 162...\n",
      "Train MSE = 0.59886     Validation MSE = 0.65411\n",
      "Fitting epoch 163...\n",
      "Train MSE = 0.59880     Validation MSE = 0.65412\n",
      "Fitting epoch 164...\n",
      "Train MSE = 0.59873     Validation MSE = 0.65412\n",
      "Fitting epoch 165...\n",
      "Train MSE = 0.59867     Validation MSE = 0.65412\n",
      "Fitting epoch 166...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m, in \u001b[0;36mMFRecommender.fit\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfe_train:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_weights\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 63\u001b[0m         diff \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m-\u001b[39mpred[i,j])\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame_weights[i,l] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr\u001b[38;5;241m*\u001b[39mt_user_weights[l,j]\u001b[38;5;241m*\u001b[39mdiff\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_weights[l,j] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr\u001b[38;5;241m*\u001b[39mt_game_weights[i,l]\u001b[38;5;241m*\u001b[39mdiff\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/project_env/lib/python3.9/site-packages/pandas/core/indexing.py:2382\u001b[0m, in \u001b[0;36m_ScalarAccessIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2379\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid call for scalar access (getting)!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2381\u001b[0m key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_key(key)\n\u001b[0;32m-> 2382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf.fit(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "932037c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6756058611854633"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.valid_MSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9470062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "570             NaN\n",
       "1172470         NaN\n",
       "730        4.721250\n",
       "578080     3.896636\n",
       "1063730         NaN\n",
       "             ...   \n",
       "335330          NaN\n",
       "21090           NaN\n",
       "1259970         NaN\n",
       "57900           NaN\n",
       "337950          NaN\n",
       "Name: 76561198153524465, Length: 986, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_df[log_df.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48385566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: [0.37898438]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "570        4.130300\n",
       "1172470    3.358850\n",
       "730        4.792823\n",
       "578080     4.105971\n",
       "1063730    4.060734\n",
       "             ...   \n",
       "335330     1.654212\n",
       "21090      2.239318\n",
       "1259970    2.551546\n",
       "57900      2.499943\n",
       "337950     1.673353\n",
       "Name: 76561198153524465, Length: 986, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(log_df[log_df.columns[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d84bb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data seems to be normally distributed by log. \n",
    "\n",
    "tlist = []\n",
    "for i in range(len(clf.filled_entries)):\n",
    "    j, k = clf.filled_entries[i]\n",
    "    tlist.append(log_df.iat[j, k])\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(tlist, bins=100)\n",
    "#plt.yscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
